{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a47c09-85bd-455a-b99f-460359495c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, EncoderDecoderModel, BertForMaskedLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f00e48a-ddc1-4cff-8606-f0b8a587bc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6aa867-657c-4565-8ee6-e70664f412b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74cf28b1-1580-4ff8-b2dd-c97ca8d6ec2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoderConfig {\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"bert-base-uncased\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.2.1\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 30522\n",
      "  },\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"bert-base-uncased\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.2.1\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 30522\n",
      "  },\n",
      "  \"encoder_no_repeat_ngram_size\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": null,\n",
      "  \"forced_eos_token_id\": null,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 256,\n",
      "  \"min_length\": 32,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": null,\n",
      "  \"remove_invalid_values\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"torch_dtype\": null,\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model and check point\n",
    "MODEL_PATH = './models/bert2bert.pt'\n",
    "\n",
    "\n",
    "# load model and check point\n",
    "bert2bert = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "bert2bert.config.decoder_start_token_id = 1\n",
    "bert2bert.config.eos_token_id = 2\n",
    "bert2bert.config.sep_token_id = 3\n",
    "bert2bert.config.pad_token_id = 0\n",
    "\n",
    "# sensible parameters for beam search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 256\n",
    "bert2bert.config.min_length = 32\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4\n",
    "\n",
    "print(bert2bert.config) # __dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653cfe28-d4bc-4aae-ae68-a8bb23de29e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "clss tensor([[   0,   27,   82,  135,  156,  186,  194,  250,  267,  287,  313,  354,\n",
      "          369,  425,  446,  473,  524,  558,  602,  630,  695,  732,  750,  774,\n",
      "          790,  856,  892,  937,  966,  992, 1017, 1065, 1080, 1119, 1156, 1186,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1],\n",
      "        [   0,    9,   19,   29,   50,   84,  118,  144,  172,  186,  222,  242,\n",
      "          255,  290,  319,  337,  373,  410,  426,  435,  480,  521,  529,  553,\n",
      "          567,  608,  627,  648,  667,  703,  720,  746,  761,  795,  828,  844,\n",
      "          866,  888,  929,  971,  986, 1024, 1067, 1085, 1121, 1149, 1171,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1],\n",
      "        [   0,   10,   20,   73,  114,  155,  173,  197,  233,  256,  276,  308,\n",
      "          332,  341,  354,  377,  409,  447,  460,  475,  494,  508,  542,  576,\n",
      "          610,  634,  672,  709,  739,  764,  812,  849,  878,  894,  935,  957,\n",
      "          990, 1016, 1027, 1040, 1084, 1099, 1117, 1136, 1153, 1174, 1189, 1211,\n",
      "         1227, 1253, 1265, 1290, 1310, 1337, 1354, 1383, 1403, 1416, 1442, 1486,\n",
      "         1507, 1535, 1545, 1557, 1573]])\n",
      "token_len_src tensor([1227, 1195, 1604])\n",
      "token_len_tgt tensor([ 55, 187,  36])\n",
      "sent_len tensor([36, 47, 65])\n",
      "padding_token tensor([-1, -1, -1])\n",
      "segs tensor([[ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])\n",
      "src tensor([[  101,  2002,  1005,  ...,    -1,    -1,    -1],\n",
      "        [  101,  3679,  5653,  ...,    -1,    -1,    -1],\n",
      "        [  101,  2403, 29627,  ...,  5473,  1012,   102]])\n",
      "src_sent_labels tensor([[ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tgt tensor([[    1,  4654, 29624,  2368,  6767,  2100,  5927,  5751,  2008, 13619,\n",
      "          6968,  4492, 17994,  1000,  2003,  2025,  1999,  2204,  3061,  2007,\n",
      "          2010,  2783,  3677,  1000,     3, 13619,  6968,  4492, 17994,  1998,\n",
      "          2845,  2343,  8748, 22072,  2031,  5720,  2011,  3042,  2021,  2031,\n",
      "          2025,  2777,     3,  1996, 28368,  2343, 16783,  4474,  2008, 22072,\n",
      "          2038,  2025,  2589,  2062,     2,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1],\n",
      "        [    1,  2321,  2566,  9358,  1997,  2216,  4793,  2385,  2000,  2484,\n",
      "          2040,  2031,  4445,  4624,  4496,  3105,  1010,  2429,  2000,  3189,\n",
      "          2011,  1996,  1996,  4495,  3842,  6056,     3, 18373,  2402,  6001,\n",
      "          2024,  4394,  2041,  2006,  1037,  3332,  2000,  3857,  4813,  2027,\n",
      "          2097,  2342,  2101,  1999,  2166,     3,  4749,  2163,  2031,  2464,\n",
      "          3623,  1999,  1996,  2193,  1997,  2945,  2542,  1999,  5635,     3,\n",
      "          3429,  2163,  2031,  2464,  4398,  3991, 29373,  2991,  1999,  1996,\n",
      "          2197,  2095,     3,  1037,  2402,  2711,  1005,  2015,  2451,  2003,\n",
      "          2411,  4876,  5079,  2000,  2010,  2030,  2014,  3112,     3,  2163,\n",
      "          2008,  2020,  2725,  2092,  2005,  2049,  2402,  2111,  2421,  8839,\n",
      "          1010,  5135,  1998,  2167,  7734,  1012,     3,  7756,  1010,  5900,\n",
      "          1998,  2047,  3290,  2024,  2012,  1996,  3953,  1997,  1996,  2862,\n",
      "             3,  3190,  1010,  5395,  1010,  5759,  1010,  5631,  1010,  4407,\n",
      "          1010,  2047,  2259,  1010,  3050,  3349,  1010,  5865,  1998, 12497,\n",
      "          1010,  2662,  1010,  2035,  2031,  2062,  2084,  2531, 29623,  8889,\n",
      "          2692, 18373,  3360,     3,  1999,  5900,  1998,  2225,  3448,  1015,\n",
      "          1999,  1019,  2402,  2111,  2024, 18373,     3,  5900,  2038,  2019,\n",
      "          3452, 12163,  3446,  1997,  1022,  2566,  9358,  1010,  2096,  2225,\n",
      "          3448,  8466,  2055,  1021,  2566,  9358,     2],\n",
      "        [    1, 14766,  2024,  2025,  2559,  2005,  2019,  2648,  6359,  2030,\n",
      "          2012, 29624,  8017,  3351,  8343,     3,  2610,  2056,  5904,  8040,\n",
      "          7100,  2075,  1010,  4601,  1010,  1998,  2014,  2365,  7291,  1010,\n",
      "          2184,  1010,  2020,  2025,  4457,     2,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1]])\n"
     ]
    }
   ],
   "source": [
    "from data import batch2BertPadId, createAttMask, collate_fn, SummarisationDataset\n",
    "from text2token import RawDocTokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30db7694-40ae-4103-b78c-2dda16f96204",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx =  2683\n",
      "'Exploited by criminals': Gambian footballer Baboucarr Ceesay (above) was among the 900 migrants who died in the Mediterranean boat disaster, his British aunt has revealed . A British woman has revealed how her nephew was among the 900 migrants who drowned in the Mediterranean boat disaster. Baboucarr Ceesay, a talented footballer from The Gambia, is believed to have died on the fishing boat in a 'desperate' attempt to seek a new life in the UK. His aunt Jessica Sey, from Cheltenham, has spoken of her devastation after discovering that he was not among the 27 survivors and demanded the human traffickers be brought to justice. She said: 'He had his head turned and his money taken by criminals who are responsible for thousands of deaths. It's the biggest shock of my life. 'His mother will never get his body back. She'll never be able to ask him why he did it.' She said her two daughters were 'absolutely gobsmacked' by news of the 21-year-old's death. Mrs Sey and her Gambian-born husband lived in the west African country for around five years. She said her nephew played first division football in The Gambia, was the eldest of four siblings and appeared to enjoy a comfortable life. She told the Gloucestershire Echo: 'His mother, Jabou, has lost two husbands and had a really tough time through no fault of her own. 'Nobody is surprised that Baboucarr wanted to make the most of his life but there was absolutely no need for him to do this. 'He wanted to come to Europe but he would never have been granted a visa. In his young head, the logical way out was to do it illegally.' Mr Ceesay and a friend, Alagie, bought a car and then 'just disappeared', she added. Horror: Jessica Sey, from Cheltenham, has spoken of her devastation after discovering that her nephew Baboucarr Ceesay was among the 900 migrants who drowned in the Mediterranean boat disaster . The pair then took a boat from the Gambian capital Banjul to Guinea, where they  crossed the Sahara and made their way to a Libyan migrant camp. Because they had money in their pockets, they didn't stay in the camp for long and were pushed to the front of the queue. But when the boat arrived, a riot broke out in the camp and everyone fought for a place on the boat. Mr Ceesay managed to get on and was placed one of the holds, possibly becoming one of those who were locked in below deck, unable to escape. Alagie was left behind. Some 900 people are feared to have drowned when the overcrowded boat capsized off the coast of Libya on Sunday in the worst maritime disaster since the end of World War Two. When news arrived that the boat had sunk, Alagie had to call Mr Ceesay's family. Tragedy: Members of the crew Italian Coast Guard ship Gregoretti carry bodies of some of the 900 victims from the Mediterranean's migrant boat tragedy in Valletta's Grand Harbour, Malta, earlier this week . Mrs Sey said: 'He's left a huge, very loving family absolutely distraught. 'It's so easy for us in our ivory towers to criticise them, but we have no idea what their lives are like. Barbaric human traffickers are separating migrants by race, locking those with the darkest skin below deck where death is inevitable if the boat sinks, survivors have told. Teenagers arriving on the Italian island of Lampedusa told how passengers from sub-Saharan Africa were placed in the hold with no water, sunlight or, crucially, means of escape. One boy from Somalia told aid workers from Save The Children: 'The Libyans who got me to Italy are not human. 'They pushed eight Nigerians into the sea. They all drowned.' Conversely, Yusuf, a 17-year-old Palestinian boy who fled Gaza for Italy in February, said he was placed on an upper level with other people from the Middle East, it was reported by The Independent. 'Even Baboucarr, who was relatively well off by Gambian standards.' Alagie is still in Libya and his family is desperate for him to return home safely. European governments were today under mounting pressure to act decisively on the Mediterranean migrant crisis as harrowing details emerged of the fate of those who died in the worst tragedy to date. With the crisis set to be discussed at an emergency summit today, allegations of callous disregard for Arab and African lives are pushing EU leaders to respond to a disaster. The vast majority of those on board were locked in the hold or the middle deck of the 20-metre (66-foot) boat when it capsized following a collision with a Portuguese cargo ship responding to its distress signal. Only 27, including two crew members who have been arrested, survived and 24 bodies have been recovered. Charged with manslaughter: Tunisian boat captain Mohammed Ali Malek (centre) is seen speaking to a nurse on an Italian coastguard ship before being arrested over the deaths of 900 migrants . A police handout showing Mohammed Ali Malek (left) and Mahmud Bikhit (right) after their arrest in Malta . Survivors have described the moment the doomed ship sank, telling how how they clung to dead bodies in the pitch black sea in order to stay afloat. They also accused the captain of being drunk moments before he rammed another ship, causing the tragedy. Bangladeshi Riajul Islam, 17, said the only reason he survived was because he was on the top deck and knew how to swim. Nasir Khan, a fellow 17-year-old Bangladeshi, said he was also on the top deck and lived by clinging on to a life vest. The stricken boat initially set off from Egypt and then stopped off on the Libyan coast near the city Zuwarah to pick up more passengers, it has been reported. It set off from Libya on Saturday and sent out a distress signal shortly before midnight 120 miles south of the Italian island of Lampedusa. It later capsized.\n"
     ]
    }
   ],
   "source": [
    "# random.seed(26)\n",
    "# load test data file, get one random abstract and summary\n",
    "pd_test = pd.read_csv('./cnn_dailymail/test.csv')\n",
    "idx = random.randint(0, pd_test.shape[0]-1)\n",
    "\n",
    "article = pd_test.iloc[idx]['article']\n",
    "\n",
    "# gt_summary = pd_test.iloc[idx]['highlights']\n",
    "print('idx = ', idx)\n",
    "print(article)\n",
    "# print(gt_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6564150-3770-4d98-93e0-009dcebf9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fde1b89-4d39-4d6d-84c4-06c4aebb4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process after generating\n",
    "def format_summary(raw_summary: str) -> str:\n",
    "    summary = (raw_summary.replace(\"[unused0]\", \"\")\n",
    "                          .replace(\"[unused3]\", \"\")\n",
    "                          .replace(\"[PAD]\", \"\")\n",
    "                          .replace(\"[unused1]\", \"\")\n",
    "                          .replace(r\" +\", \" \")\n",
    "                          .replace(\" [unused2] \", \". \")\n",
    "                          .replace(\"[unused2]\", \"\")\n",
    "                          .replace(\" .\", \".\")\n",
    "                          .replace(\" ,\", \",\")\n",
    "                          .replace(\" ?\", \"?\")\n",
    "                          .replace(\" !\", \"!\")\n",
    "                          .strip() )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3199a9-4b3f-4aa7-9b58-f9d69d657f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tienbui/virtualenvs/py38_vir/lib/python3.9/site-packages/transformers/generation_utils.py:1632: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED OUTPUT:\n",
      "baboucarr ceesay was among 900 migrants who drowned in the mediterranean boat disaster. his aunt jessica sey has spoken of her devastation after discovering he was not among 27 survivors. she says her nephew's death was \" absolutely gobsmacked \" by news of his death. babou, 21, was the eldest of four siblings from the west african football players\n"
     ]
    }
   ],
   "source": [
    "# generate one summary from a document\n",
    "def generate_summary(article: str):\n",
    "    bert2bert.eval()\n",
    "    rdt = RawDocTokenize(article, ' ',\n",
    "                         article_max_token_length = 512,\n",
    "                         summary_max_token_length = 512)\n",
    "\n",
    "    preprocessed1 = rdt.get_tokenized_output(padding = True)\n",
    "\n",
    "    src = torch.tensor(preprocessed1['src'], dtype = torch.long).view(1, -1)\n",
    "#     print(src.shape)\n",
    "\n",
    "    # convert -1 to padding token\n",
    "    src = batch2BertPadId(src)\n",
    "    # get attention mask\n",
    "    src_att_mask = createAttMask(src)\n",
    "    src, src_att_mask = src.to(device), src_att_mask.to(device)\n",
    "\n",
    "    # get output as token id\n",
    "    outputs = bert2bert.generate(src, attention_mask = src_att_mask, decoder_start_token_id =1)\n",
    "    # token id to string by tokenzier.batch_decoder\n",
    "    pred_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(pred_str)\n",
    "    pred_str_formated = format_summary(pred_str[0])\n",
    "\n",
    "    # replace special character\n",
    "    pred_str_formated = pred_str_formated.replace('[unused0] ', '')\n",
    "    pred_str_formated = pred_str_formated.replace('[unused1]', '.')\n",
    "    pred_str_formated = pred_str_formated.replace('[unused2] ', '.')\n",
    "\n",
    "#     print(pred_str_formated)\n",
    "    return pred_str_formated\n",
    "\n",
    "pred_summary = generate_summary(article)\n",
    "print('PREDICTED OUTPUT:')\n",
    "print(pred_summary)\n",
    "# print('GROUND TRUTH ABSTRACTIVE SUMMARY:')\n",
    "# print(gt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0662da-795d-4b38-8a5e-6cccde00f060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_vir",
   "language": "python",
   "name": "py38_vir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
