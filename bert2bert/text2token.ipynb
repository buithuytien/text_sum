{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369826a4-bd3c-482d-b4c6-9307a29bf0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "# from modeling_bertabs import BertAbsConfig, BertAbs, build_predictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90807f7c-f8f7-4e81-b09e-975c106f1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = './cnn_dailymail'\n",
    "train_fname = RAW_DATA_DIR + '/train.csv'\n",
    "test_fname = RAW_DATA_DIR + '/train.csv'\n",
    "valid_fname = RAW_DATA_DIR + '/train.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5fab78e-781c-415f-afda-0cbeed34d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDocTokenize():\n",
    "    '''\n",
    "    to tokenize the article and summary text into bert token id\n",
    "    \n",
    "    usage: \n",
    "        rdt = RawDocTokenize(raw_article, raw_summary)\n",
    "        tokenized_output = rdt.get_tokenized_output()\n",
    "        \n",
    "    input:\n",
    "        raw_article, raw_summary: string of unprocessed text\n",
    "    \n",
    "    output of rdt.get_tokenized_output():\n",
    "        dictionary with keys: ['src', 'tgt', 'segs', 'clss']\n",
    "            src: bert token ids of article\n",
    "            tgt: bert token ids of summary\n",
    "            segs: sentence embeddings [000000111111000111] mark sentences\n",
    "            clss: position of cls token (sentence begin token in src)\n",
    "    '''\n",
    "    def __init__(self, raw_article, raw_summary, \n",
    "                 article_min_sentence_length = 6,\n",
    "                 summary_min_sentence_length = 5,\n",
    "                 article_max_token_length = None,\n",
    "                 summary_max_token_length = None):\n",
    "        '''\n",
    "        raw_article, raw_summary: string of unprocessed text\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        self.raw_article = raw_article\n",
    "        self.raw_summary = raw_summary\n",
    "        self.article_min_sentence_length = article_min_sentence_length\n",
    "        self.summary_min_sentence_length = summary_min_sentence_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower=True)\n",
    "        \n",
    "        # max_token_length is only used when padding = True. For Bert work\n",
    "        if article_max_token_length is not None:\n",
    "            self.article_max_token_length = article_max_token_length\n",
    "        else :\n",
    "            self.article_max_token_length = 512 #article_max_token_length\n",
    "        \n",
    "        if summary_max_token_length is not None:\n",
    "            self.summary_max_token_length = summary_max_token_length\n",
    "        else:\n",
    "            self.summary_max_token_length = 512 #summary_max_token_length\n",
    "        \n",
    "        \n",
    "    def add_missing_period(self, line):\n",
    "        END_TOKENS = [\".\", \"!\", \"?\", \"...\", \"'\", \"`\", '\"', u\"\\u2019\", u\"\\u2019\", \")\"]\n",
    "        if line.startswith(\"@highlight\"):\n",
    "            return line\n",
    "        if line[-1] in END_TOKENS and len(line):\n",
    "            return line\n",
    "        return line + \".\"\n",
    "    \n",
    "    def raw_text_process(self):\n",
    "        article = self.raw_article\n",
    "        article = [s.strip()+'.' for s in article.split('.')]\n",
    "        article = [self.add_missing_period(line) for line in article if len(line) > 0]\n",
    "        # remove sentences that are too short\n",
    "        article = [s for s in article if s not in ['..', '.'] and len(s.split(' ')) >= self.article_min_sentence_length]\n",
    "        \n",
    "        summary = self.raw_summary\n",
    "        summary = [s.strip()+'.' for s in summary.split('.')]\n",
    "        summary = [self.add_missing_period(line) for line in summary if len(line) > 0]\n",
    "        # remove sentences that are too short\n",
    "        summary = [s for s in summary if s not in ['..', '.'] and len(s.split(' ')) >= self.summary_min_sentence_length]\n",
    "        \n",
    "        print('from raw text process:')\n",
    "        print('article list broken = ', article[:13])\n",
    "        print('summary list broken = ', summary[:4])\n",
    "        \n",
    "        self.article = article\n",
    "        self.summary = summary\n",
    "        return article, summary\n",
    "    \n",
    "    def tokenize_article(self, padding: bool):\n",
    "        '''\n",
    "        add CLS and SEP token at sentence boundary     \n",
    "        '''\n",
    "#         min_sentence_length = self.article_min_sentence_length\n",
    "#         article = [item for item in self.article if len(item.split(' ')) >= min_sentence_length]\n",
    "        \n",
    "        encoding = self.tokenizer(self.article)\n",
    "        src = [item for sublist in encoding['input_ids'] \n",
    "                    for item in sublist  ]\n",
    "        segs = []\n",
    "        for i in range(len(encoding['token_type_ids'])):\n",
    "            new_sub_token_type_ids = [ segi + (i%2) for segi in encoding['token_type_ids'][i]]\n",
    "            segs += new_sub_token_type_ids\n",
    "        \n",
    "        cls_tokenid = self.tokenizer.vocab['[CLS]'] \n",
    "        \n",
    "        # pad the remaining of the sentences if len[src] < 512\n",
    "        if padding:\n",
    "            max_token_len = self.article_max_token_length\n",
    "            if len(src) > max_token_len:\n",
    "                src = src[:max_token_len]\n",
    "                segs = segs[:max_token_len]\n",
    "            else:\n",
    "                src += [-1]*(max_token_len - len(src))\n",
    "                segs += [-1]*(max_token_len - len(src))\n",
    "            \n",
    "        clss = [i for i, tokenid in enumerate(src) if tokenid == cls_tokenid ] # position of CLS token (101)\n",
    "        \n",
    "        self.src = src\n",
    "        self.segs = segs\n",
    "        self.clss = clss\n",
    "        return src, segs, clss\n",
    "        \n",
    "        \n",
    "    def tokenize_summary(self, padding: bool):\n",
    "#         min_sentence_length = self.summary_mn_sentence_length\n",
    "#         summary = [item for item in self.summary if len(item.split(' ')) >= min_sentence_length]\n",
    "        tgt_encoding = self.tokenizer(self.summary)\n",
    "        decoder_symbols = {\n",
    "            \"BOS\": self.tokenizer.vocab[\"[unused0]\"],  # 1\n",
    "            \"EOS\": self.tokenizer.vocab[\"[unused1]\"],  # 2\n",
    "            \"TRG_SENT_SPLIT\": self.tokenizer.vocab[\"[unused2]\"],\n",
    "            \"PAD\": self.tokenizer.vocab[\"[PAD]\"]       # 0\n",
    "        }\n",
    "\n",
    "        tgt = [item for sublist in tgt_encoding['input_ids'] for item in sublist]\n",
    "        for i, tokenid in enumerate(tgt):\n",
    "            if tokenid == 101: # CLS:\n",
    "                tgt[i] = decoder_symbols['BOS']\n",
    "            elif tokenid == 102:\n",
    "                tgt[i] = decoder_symbols['TRG_SENT_SPLIT']\n",
    "\n",
    "        tgt[-1] = decoder_symbols['EOS']\n",
    "        \n",
    "        # pad the remaining of the sentences if len[src] < 512\n",
    "        if padding:\n",
    "            max_token_len = self.summary_max_token_length\n",
    "            if len(tgt) > max_token_len:\n",
    "                tgt = tgt[:max_token_len]\n",
    "            else:\n",
    "                tgt += [-1]*(max_token_len - len(tgt))\n",
    "        \n",
    "        self.tgt = tgt\n",
    "        return tgt\n",
    "        \n",
    "    \n",
    "    def get_tokenized_output(self, padding: bool = False):\n",
    "        key_names = ['src', 'tgt', 'segs', 'clss', 'src_txt', 'tgt_txt']\n",
    "        article, summary = self.raw_text_process()\n",
    "        _ = self.tokenize_article(padding)\n",
    "        _ = self.tokenize_summary(padding)\n",
    "        return {'src': self.src,\n",
    "                'tgt': self.tgt,\n",
    "                'segs': self.segs,\n",
    "                'clss': self.clss }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c427d0b0-f55f-4a07-9e38-e890bf857530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n"
     ]
    }
   ],
   "source": [
    "train_pd = pd.read_csv(train_fname)[:100]\n",
    "print(train_pd.iloc[0]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da43b241-c677-4ab0-9b94-4b0930a81d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx =  4\n",
      "from raw text process:\n",
      "article list broken =  ['Fleetwood are the only team still to have a 100% record in Sky Bet League One as a 2-0 win over Scunthorpe sent Graham Alexander’s men top of the table.', 'The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park.', 'Fleetwood were one of five teams to have won two out of two but the other four clubs - Peterborough, Bristol City, Chesterfield and Crawley - all hit their first stumbling blocks.', 'Posh were defeated 2-1 by Sheffield United, who had lost both of their opening contests.', 'Jose Baxter’s opener gave the Blades a first-half lead, and although it was later cancelled out by Shaun Brisley’s goal, Ben Davies snatched a winner six minutes from time.', 'In the lead: Jose Baxter (right) celebrates opening the scoring for Sheffield United.', \"Up for the battle: Sheffield United's Michael Doyle (left) challenges Peterborough's Kyle Vassell in a keenly-contested clash.\", \"Bristol City, who beat Nigel Clough’s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven days’ time.\", 'Arsenal loanee Benik Afobe scored the only goal of the game just after the break.', 'Meanwhile, Crawley lost their unbeaten status, while Bradford maintained theirs, thanks to a 3-1 win for the Bantams.', 'James Hanson became the first player to score against Crawley this season after 49 minutes before Joe Walsh equalised five minutes later.', \"Heads up: Bristol City's Korey Smith (left) and Leyton Orient's Lloyd James go up for a header.\", \"But strikes from Billy Knott and Mason Bennett sealed an impressive away win Phil Parkinson's men.\"]\n",
      "summary list broken =  ['Fleetwood top of League One after 2-0 win at Scunthorpe.', 'Peterborough, Bristol City, Chesterfield and Crawley all drop first points of the season.', 'Stand-in striker Matt Done scores a hat-trick as Rochdale thrash Crewe 5-2.', 'Wins for Notts County and Yeovil.']\n",
      "src [101, 23866, 2024, 1996, 2069, 2136, 2145, 2000, 2031, 1037, 2531, 1003, 2501, 1999, 3712, 6655, 2223, 2028, 2004, 1037, 1016, 1011, 1014, 2663, 2058, 8040, 16671, 22044, 2741, 5846, 3656, 1521, 1055, 2273, 2327, 1997, 1996, 2795, 1012, 102, 101, 1996, 19429, 2390, 2024, 2652, 1999, 1996, 2353, 7563, 2005, 1996, 2034, 2051, 1999, 2037, 2381, 2044, 2416, 15365, 1999, 3157, 2086, 1998, 2037, 9487, 16354, 3065, 2053, 3696, 1997, 18068, 2007, 6175, 28770, 1998, 20243, 6473, 4577, 1996, 3289, 2012, 1043, 5802, 3877, 2380, 1012, 102, 101, 23866, 2020, 2028, 1997, 2274, 2780, 2000, 2031, 2180, 2048, 2041, 1997, 2048, 2021, 1996, 2060, 2176, 4184, 1011, 17587, 1010, 7067, 2103, 1010, 22699, 1998, 28854, 1011, 2035, 2718, 2037, 2034, 19730, 5991, 1012, 102, 101, 13433, 4095, 2020, 3249, 1016, 1011, 1015, 2011, 8533, 2142, 1010, 2040, 2018, 2439, 2119, 1997, 2037, 3098, 15795, 1012, 102, 101, 4560, 14664, 1521, 1055, 16181, 2435, 1996, 10491, 1037, 2034, 1011, 2431, 2599, 1010, 1998, 2348, 2009, 2001, 2101, 8014, 2041, 2011, 16845, 7987, 2483, 3051, 1521, 1055, 3125, 1010, 3841, 9082, 14177, 1037, 3453, 2416, 2781, 2013, 2051, 1012, 102, 101, 1999, 1996, 2599, 1024, 4560, 14664, 1006, 2157, 1007, 21566, 3098, 1996, 4577, 2005, 8533, 2142, 1012, 102, 101, 2039, 2005, 1996, 2645, 1024, 8533, 2142, 1005, 1055, 2745, 11294, 1006, 2187, 1007, 7860, 17587, 1005, 1055, 7648, 12436, 21218, 2140, 1999, 1037, 10326, 2135, 1011, 7259, 13249, 1012, 102, 101, 7067, 2103, 1010, 2040, 3786, 12829, 18856, 10593, 1521, 1055, 2273, 2006, 1996, 3098, 2154, 1010, 2020, 2218, 2000, 1037, 3125, 3238, 4009, 2011, 2197, 2161, 1005, 1055, 2377, 1011, 2125, 13527, 25866, 2669, 16865, 2096, 22699, 1010, 1996, 2223, 2048, 3966, 1010, 2020, 7854, 1015, 1011, 1014, 2011, 12395, 2123, 2015, 1010, 2040, 2377, 5087, 2142, 1999, 1996, 3007, 2028, 2452, 1999, 2698, 2420, 1521, 2051, 1012, 102, 101, 9433, 5414, 4402, 3841, 5480, 21358, 20891, 3195, 1996, 2069, 3125, 1997, 1996, 2208, 2074, 2044, 1996, 3338, 1012, 102, 101, 5564, 1010, 28854, 2439, 2037, 20458, 3570, 1010, 2096, 9999, 5224, 17156, 1010, 4283, 2000, 1037, 1017, 1011, 1015, 2663, 2005, 1996, 7221, 15464, 2015, 1012, 102, 101, 2508, 17179, 2150, 1996, 2034, 2447, 2000, 3556, 2114, 28854, 2023, 2161, 2044, 4749, 2781, 2077, 3533, 11019, 5020, 5084, 2274, 2781, 2101, 1012, 102, 101, 4641, 2039, 1024, 7067, 2103, 1005, 1055, 12849, 15202, 3044, 1006, 2187, 1007, 1998, 25866, 2669, 16865, 1005, 1055, 6746, 2508, 2175, 2039, 2005, 1037, 20346, 1012, 102, 101, 2021, 9326, 2013, 5006, 12226, 2102, 1998, 6701, 8076, 10203, 2019, 8052, 2185, 2663, 6316, 20310, 1005, 1055, 2273, 1012, 102, 101, 9999, 2024, 2085, 2117, 2369, 23866, 2044, 18895, 1521, 1055, 2644, 13704, 1011, 2051, 5020, 17288, 3214, 8475, 1010, 2005, 3183, 3533, 18661, 2772, 1037, 2047, 3206, 3041, 2006, 9857, 1010, 2020, 2218, 2000, 1037, 1015, 1011, 1015, 4009, 2029, 5707, 2068, 2091, 1996, 2795, 1012, 102, 101, 3782, 15462, 2246, 2000, 2031, 7119, 1996, 2685, 2005, 1996, 7094, 2860, 16584, 2229, 2021, 7150, 19356, 4930, 1037, 2197, 1011, 12008, 2504, 3917, 1012, 102, 101]\n",
      "tgt [1, 23866, 2327, 1997, 2223, 2028, 2044, 1016, 1011, 1014, 2663, 2012, 8040, 16671, 22044, 1012, 3, 1, 17587, 1010, 7067, 2103, 1010, 22699, 1998, 28854, 2035, 4530, 2034, 2685, 1997, 1996, 2161, 1012, 3, 1, 3233, 1011, 1999, 11854, 4717, 2589, 7644, 1037, 6045, 1011, 7577, 2004, 26109, 27042, 22188, 1019, 1011, 1016, 1012, 3, 1, 5222, 2005, 2025, 3215, 2221, 1998, 6300, 4492, 4014, 1012, 3, 1, 13613, 1013, 9999, 1998, 18285, 1013, 3417, 10380, 2119, 2203, 1999, 9891, 1012, 3, 1, 1037, 2397, 4459, 2011, 5880, 2219, 3125, 23439, 12267, 16445, 2093, 2685, 2114, 4971, 9628, 1012, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "segs [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "clss [0, 40, 88, 125, 147, 189, 208, 240, 310, 331, 359, 385, 414, 436, 484, 511]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, train_pd.shape[0]-1)\n",
    "print('idx = ', idx)\n",
    "# idx=0\n",
    "raw_article0, raw_summary0 = train_pd.iloc[idx]['article'], train_pd.iloc[idx]['highlights']\n",
    "\n",
    "rdt = RawDocTokenize(raw_article0, raw_summary0,\n",
    "                     article_max_token_length = 512,\n",
    "                     summary_max_token_length = 512)\n",
    "# article0, summary0 = rdt.raw_text_process()\n",
    "# src, segs, clss = rdt.tokenize_article()\n",
    "# print(src)\n",
    "# print(segs)\n",
    "# print(clss)\n",
    "\n",
    "# tgt = rdt.tokenize_summary()\n",
    "\n",
    "preprcessed1 = rdt.get_tokenized_output(padding = True)\n",
    "for k, v in preprcessed1.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520534e1-e8c3-4164-95e9-380c92b8099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all text to hdf5 file, for further training\n",
    "\n",
    "def text2npdic(fname):\n",
    "    train_pd = pd.read_csv(fname)\n",
    "\n",
    "    max_sentence_length = 512\n",
    "    merged_np = np.zeros((train_pd.shape[0], max_sentence_length))\n",
    "    merged_train = {}\n",
    "\n",
    "\n",
    "    for idx in range(len(train_pd.shape[0]))\n",
    "        raw_article0, raw_summary0 = train_pd.iloc[idx]['article'], train_pd.iloc[idx]['highlights']\n",
    "        rdt = RawDocTokenize(raw_article0, raw_summary0)\n",
    "        out = rdt.get_tokenized_output(padding = True)\n",
    "        if idx == 0: # initialize dictionary\n",
    "            for k in out.keys():\n",
    "                merged_train[k] = merged_np\n",
    "\n",
    "        for k, v in out.items():\n",
    "            merged_train[k][idx, :] = v\n",
    "        \n",
    "    return merged_train\n",
    "\n",
    "merged_train = text2npdic(train_fname)\n",
    "with h5py.File(\"./preprocessed/train_data.hdf5\", \"w\") as data_file:\n",
    "    for k, v in merged_train.item():\n",
    "        data_file.create_dataset(name = k, data=v)\n",
    "data_file.close()\n",
    "\n",
    "merged_test = text2npdic(test_fname)\n",
    "with h5py.File(\"./preprocessed/test_data.hdf5\", \"w\") as data_file:\n",
    "    for k, v in merged_test.item():\n",
    "        data_file.create_dataset(name = k, data=v)\n",
    "data_file.close()\n",
    "\n",
    "merged_valid = text2npdic(train_fname)\n",
    "with h5py.File(\"./preprocessed/valid_data.hdf5\", \"w\") as data_file:\n",
    "    for k, v in merged_valid.item():\n",
    "        data_file.create_dataset(name = k, data=v)\n",
    "data_file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_vir",
   "language": "python",
   "name": "py38_vir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
