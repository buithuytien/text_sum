{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebc34a7-fadf-4ae9-ab60-a4f9b91ed7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, EncoderDecoderModel, BertForMaskedLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43b34da-2e64-48e8-9ff3-b7b60888ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf16878c-b3fa-4a34-9636-190c7d80d9c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert2bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50339/3220477575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert2bert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bert2bert' is not defined"
     ]
    }
   ],
   "source": [
    "print(bert2bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0957823-92ff-4b1d-893a-ff4b6652bce1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# bert2bert\n",
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "BertLMHeadModel_new_initialized_weights = ['bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight']\n",
    "bert2bert_new_initialized_weights = ['decoder.' + BertLMHeadModel_new_initialized_weights[i] for i in range(len(BertLMHeadModel_new_initialized_weights))]\n",
    "# print(bert2bert_new_initialized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be5a47-52aa-493f-9581-b764bc566498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = BertModel.from_pretrained(\"bert-base-uncased\") # config=encoder_config\n",
    "# decoder = BertForMaskedLM.from_pretrained(\"bert-base-uncased\") #config=decoder_config\n",
    "# model = EncoderDecoderModel(encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b181ca2-daa7-4e19-b41a-deef4d8f8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "new_bert2bert = copy.deepcopy(bert2bert)\n",
    "# old_bert2bert = copy.deepcopy(bert2bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02032b3e-c398-4c95-8a07-8141c81a68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_bert2bert.decoder.bert.encoder.layer)\n",
    "NUM_LAYER_DECODER = 4\n",
    "new_bert2bert.decoder.bert.encoder.layer = new_bert2bert.decoder.bert.encoder.layer[:NUM_LAYER_DECODER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a97374e7-9f31-438e-976a-5c83fd8811ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_bert2bert.decoder.bert.encoder.layer)\n",
    "bert2bert = new_bert2bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe8ecd6-7319-4941-9881-d119e7cef160",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 247363386\n",
      "247363386\n",
      "encoder.embeddings.word_embeddings.weight 23440896\n",
      "encoder.embeddings.position_embeddings.weight 393216\n",
      "encoder.embeddings.token_type_embeddings.weight 1536\n",
      "encoder.embeddings.LayerNorm.weight 768\n",
      "encoder.embeddings.LayerNorm.bias 768\n",
      "encoder.encoder.layer.0.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.0.attention.self.query.bias 768\n",
      "encoder.encoder.layer.0.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.0.attention.self.key.bias 768\n",
      "encoder.encoder.layer.0.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.0.attention.self.value.bias 768\n",
      "encoder.encoder.layer.0.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.0.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.0.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.0.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.0.output.dense.weight 2359296\n",
      "encoder.encoder.layer.0.output.dense.bias 768\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.1.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.1.attention.self.query.bias 768\n",
      "encoder.encoder.layer.1.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.1.attention.self.key.bias 768\n",
      "encoder.encoder.layer.1.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.1.attention.self.value.bias 768\n",
      "encoder.encoder.layer.1.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.1.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.1.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.1.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.1.output.dense.weight 2359296\n",
      "encoder.encoder.layer.1.output.dense.bias 768\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.2.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.2.attention.self.query.bias 768\n",
      "encoder.encoder.layer.2.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.2.attention.self.key.bias 768\n",
      "encoder.encoder.layer.2.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.2.attention.self.value.bias 768\n",
      "encoder.encoder.layer.2.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.2.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.2.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.2.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.2.output.dense.weight 2359296\n",
      "encoder.encoder.layer.2.output.dense.bias 768\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.3.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.3.attention.self.query.bias 768\n",
      "encoder.encoder.layer.3.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.3.attention.self.key.bias 768\n",
      "encoder.encoder.layer.3.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.3.attention.self.value.bias 768\n",
      "encoder.encoder.layer.3.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.3.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.3.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.3.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.3.output.dense.weight 2359296\n",
      "encoder.encoder.layer.3.output.dense.bias 768\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.4.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.4.attention.self.query.bias 768\n",
      "encoder.encoder.layer.4.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.4.attention.self.key.bias 768\n",
      "encoder.encoder.layer.4.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.4.attention.self.value.bias 768\n",
      "encoder.encoder.layer.4.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.4.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.4.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.4.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.4.output.dense.weight 2359296\n",
      "encoder.encoder.layer.4.output.dense.bias 768\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.5.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.5.attention.self.query.bias 768\n",
      "encoder.encoder.layer.5.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.5.attention.self.key.bias 768\n",
      "encoder.encoder.layer.5.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.5.attention.self.value.bias 768\n",
      "encoder.encoder.layer.5.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.5.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.5.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.5.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.5.output.dense.weight 2359296\n",
      "encoder.encoder.layer.5.output.dense.bias 768\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.6.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.6.attention.self.query.bias 768\n",
      "encoder.encoder.layer.6.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.6.attention.self.key.bias 768\n",
      "encoder.encoder.layer.6.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.6.attention.self.value.bias 768\n",
      "encoder.encoder.layer.6.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.6.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.6.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.6.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.6.output.dense.weight 2359296\n",
      "encoder.encoder.layer.6.output.dense.bias 768\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.7.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.7.attention.self.query.bias 768\n",
      "encoder.encoder.layer.7.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.7.attention.self.key.bias 768\n",
      "encoder.encoder.layer.7.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.7.attention.self.value.bias 768\n",
      "encoder.encoder.layer.7.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.7.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.7.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.7.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.7.output.dense.weight 2359296\n",
      "encoder.encoder.layer.7.output.dense.bias 768\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.8.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.8.attention.self.query.bias 768\n",
      "encoder.encoder.layer.8.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.8.attention.self.key.bias 768\n",
      "encoder.encoder.layer.8.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.8.attention.self.value.bias 768\n",
      "encoder.encoder.layer.8.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.8.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.8.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.8.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.8.output.dense.weight 2359296\n",
      "encoder.encoder.layer.8.output.dense.bias 768\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.9.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.9.attention.self.query.bias 768\n",
      "encoder.encoder.layer.9.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.9.attention.self.key.bias 768\n",
      "encoder.encoder.layer.9.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.9.attention.self.value.bias 768\n",
      "encoder.encoder.layer.9.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.9.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.9.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.9.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.9.output.dense.weight 2359296\n",
      "encoder.encoder.layer.9.output.dense.bias 768\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.10.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.10.attention.self.query.bias 768\n",
      "encoder.encoder.layer.10.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.10.attention.self.key.bias 768\n",
      "encoder.encoder.layer.10.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.10.attention.self.value.bias 768\n",
      "encoder.encoder.layer.10.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.10.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.10.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.10.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.10.output.dense.weight 2359296\n",
      "encoder.encoder.layer.10.output.dense.bias 768\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.11.attention.self.query.weight 589824\n",
      "encoder.encoder.layer.11.attention.self.query.bias 768\n",
      "encoder.encoder.layer.11.attention.self.key.weight 589824\n",
      "encoder.encoder.layer.11.attention.self.key.bias 768\n",
      "encoder.encoder.layer.11.attention.self.value.weight 589824\n",
      "encoder.encoder.layer.11.attention.self.value.bias 768\n",
      "encoder.encoder.layer.11.attention.output.dense.weight 589824\n",
      "encoder.encoder.layer.11.attention.output.dense.bias 768\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias 768\n",
      "encoder.encoder.layer.11.intermediate.dense.weight 2359296\n",
      "encoder.encoder.layer.11.intermediate.dense.bias 3072\n",
      "encoder.encoder.layer.11.output.dense.weight 2359296\n",
      "encoder.encoder.layer.11.output.dense.bias 768\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight 768\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias 768\n",
      "encoder.pooler.dense.weight 589824\n",
      "encoder.pooler.dense.bias 768\n",
      "decoder.bert.embeddings.word_embeddings.weight 23440896\n",
      "decoder.bert.embeddings.position_embeddings.weight 393216\n",
      "decoder.bert.embeddings.token_type_embeddings.weight 1536\n",
      "decoder.bert.embeddings.LayerNorm.weight 768\n",
      "decoder.bert.embeddings.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.0.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.0.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.0.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.0.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.0.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.0.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.0.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.0.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.1.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.1.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.1.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.1.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.1.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.1.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.1.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.1.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.2.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.2.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.2.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.2.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.2.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.2.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.2.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.2.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.2.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.2.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.2.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.2.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.2.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.2.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.2.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.2.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.2.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.2.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.2.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.2.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.2.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.2.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.3.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.3.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.3.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.3.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.3.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.3.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.3.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.3.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.3.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.3.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.3.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.3.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.3.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.3.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.3.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.3.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.3.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.3.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.3.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.3.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.3.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.3.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.4.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.4.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.4.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.4.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.4.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.4.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.4.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.4.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.4.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.4.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.4.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.4.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.4.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.4.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.4.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.4.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.4.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.4.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.4.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.4.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.4.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.4.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.5.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.5.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.5.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.5.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.5.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.5.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.5.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.5.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.5.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.5.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.5.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.5.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.5.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.5.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.5.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.5.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.5.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.5.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.5.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.5.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.5.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.5.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.6.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.6.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.6.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.6.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.6.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.6.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.6.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.6.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.6.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.6.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.6.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.6.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.6.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.6.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.6.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.6.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.6.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.6.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.6.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.6.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.6.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.6.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.7.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.7.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.7.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.7.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.7.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.7.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.7.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.7.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.7.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.7.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.7.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.7.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.7.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.7.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.7.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.7.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.7.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.7.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.7.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.7.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.7.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.7.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.8.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.8.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.8.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.8.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.8.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.8.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.8.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.8.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.8.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.8.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.8.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.8.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.8.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.8.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.8.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.8.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.8.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.8.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.8.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.8.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.8.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.8.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.9.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.9.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.9.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.9.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.9.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.9.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.9.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.9.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.9.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.9.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.9.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.9.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.9.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.9.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.9.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.9.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.9.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.9.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.9.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.9.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.9.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.9.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.10.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.10.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.10.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.10.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.10.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.10.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.10.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.10.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.10.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.10.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.10.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.10.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.10.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.10.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.10.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.10.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.10.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.10.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.10.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.10.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.10.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.10.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.11.attention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.11.attention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.11.attention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.11.attention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.11.attention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.11.attention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.11.attention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.11.attention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.11.crossattention.self.query.weight 589824\n",
      "decoder.bert.encoder.layer.11.crossattention.self.query.bias 768\n",
      "decoder.bert.encoder.layer.11.crossattention.self.key.weight 589824\n",
      "decoder.bert.encoder.layer.11.crossattention.self.key.bias 768\n",
      "decoder.bert.encoder.layer.11.crossattention.self.value.weight 589824\n",
      "decoder.bert.encoder.layer.11.crossattention.self.value.bias 768\n",
      "decoder.bert.encoder.layer.11.crossattention.output.dense.weight 589824\n",
      "decoder.bert.encoder.layer.11.crossattention.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias 768\n",
      "decoder.bert.encoder.layer.11.intermediate.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.11.intermediate.dense.bias 3072\n",
      "decoder.bert.encoder.layer.11.output.dense.weight 2359296\n",
      "decoder.bert.encoder.layer.11.output.dense.bias 768\n",
      "decoder.bert.encoder.layer.11.output.LayerNorm.weight 768\n",
      "decoder.bert.encoder.layer.11.output.LayerNorm.bias 768\n",
      "decoder.cls.predictions.bias 30522\n",
      "decoder.cls.predictions.transform.dense.weight 589824\n",
      "decoder.cls.predictions.transform.dense.bias 768\n",
      "decoder.cls.predictions.transform.LayerNorm.weight 768\n",
      "decoder.cls.predictions.transform.LayerNorm.bias 768\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def count_parameters(model):\n",
    "    # table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    table = {}\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table[name] = params\n",
    "        total_params+=params\n",
    "    # print(table)\n",
    "    \n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params, table\n",
    "    \n",
    "# total_params, table = count_parameters(model)\n",
    "# print(total_params)\n",
    "# for k, v in table.items():\n",
    "#     print(k, v)\n",
    "    \n",
    "total_params, table = count_parameters(bert2bert)\n",
    "print(total_params)\n",
    "for k, v in table.items():\n",
    "    print(k, v)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297ca77c-afc9-426c-8b51-be001e42c5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(bert2bert_new_initialized_weights)\n",
    "'decoder.bert.encoder.layer.11.output.LayerNorm.bias'[8:]\n",
    "'decoder.bert.encoder.layer.11.output.LayerNorm.bias'[:7] =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd521cb6-f1ab-439c-8eaa-6bdf085795f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embeddings.word_embeddings.weight\n",
      "encoder.embeddings.position_embeddings.weight\n",
      "encoder.embeddings.token_type_embeddings.weight\n",
      "encoder.embeddings.LayerNorm.weight\n",
      "encoder.embeddings.LayerNorm.bias\n",
      "encoder.encoder.layer.0.attention.self.query.weight\n",
      "encoder.encoder.layer.0.attention.self.query.bias\n",
      "encoder.encoder.layer.0.attention.self.key.weight\n",
      "encoder.encoder.layer.0.attention.self.key.bias\n",
      "encoder.encoder.layer.0.attention.self.value.weight\n",
      "encoder.encoder.layer.0.attention.self.value.bias\n",
      "encoder.encoder.layer.0.attention.output.dense.weight\n",
      "encoder.encoder.layer.0.attention.output.dense.bias\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.0.intermediate.dense.weight\n",
      "encoder.encoder.layer.0.intermediate.dense.bias\n",
      "encoder.encoder.layer.0.output.dense.weight\n",
      "encoder.encoder.layer.0.output.dense.bias\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.attention.self.query.weight\n",
      "encoder.encoder.layer.1.attention.self.query.bias\n",
      "encoder.encoder.layer.1.attention.self.key.weight\n",
      "encoder.encoder.layer.1.attention.self.key.bias\n",
      "encoder.encoder.layer.1.attention.self.value.weight\n",
      "encoder.encoder.layer.1.attention.self.value.bias\n",
      "encoder.encoder.layer.1.attention.output.dense.weight\n",
      "encoder.encoder.layer.1.attention.output.dense.bias\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.intermediate.dense.weight\n",
      "encoder.encoder.layer.1.intermediate.dense.bias\n",
      "encoder.encoder.layer.1.output.dense.weight\n",
      "encoder.encoder.layer.1.output.dense.bias\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.attention.self.query.weight\n",
      "encoder.encoder.layer.2.attention.self.query.bias\n",
      "encoder.encoder.layer.2.attention.self.key.weight\n",
      "encoder.encoder.layer.2.attention.self.key.bias\n",
      "encoder.encoder.layer.2.attention.self.value.weight\n",
      "encoder.encoder.layer.2.attention.self.value.bias\n",
      "encoder.encoder.layer.2.attention.output.dense.weight\n",
      "encoder.encoder.layer.2.attention.output.dense.bias\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.intermediate.dense.weight\n",
      "encoder.encoder.layer.2.intermediate.dense.bias\n",
      "encoder.encoder.layer.2.output.dense.weight\n",
      "encoder.encoder.layer.2.output.dense.bias\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.attention.self.query.weight\n",
      "encoder.encoder.layer.3.attention.self.query.bias\n",
      "encoder.encoder.layer.3.attention.self.key.weight\n",
      "encoder.encoder.layer.3.attention.self.key.bias\n",
      "encoder.encoder.layer.3.attention.self.value.weight\n",
      "encoder.encoder.layer.3.attention.self.value.bias\n",
      "encoder.encoder.layer.3.attention.output.dense.weight\n",
      "encoder.encoder.layer.3.attention.output.dense.bias\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.intermediate.dense.weight\n",
      "encoder.encoder.layer.3.intermediate.dense.bias\n",
      "encoder.encoder.layer.3.output.dense.weight\n",
      "encoder.encoder.layer.3.output.dense.bias\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.attention.self.query.weight\n",
      "encoder.encoder.layer.4.attention.self.query.bias\n",
      "encoder.encoder.layer.4.attention.self.key.weight\n",
      "encoder.encoder.layer.4.attention.self.key.bias\n",
      "encoder.encoder.layer.4.attention.self.value.weight\n",
      "encoder.encoder.layer.4.attention.self.value.bias\n",
      "encoder.encoder.layer.4.attention.output.dense.weight\n",
      "encoder.encoder.layer.4.attention.output.dense.bias\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.intermediate.dense.weight\n",
      "encoder.encoder.layer.4.intermediate.dense.bias\n",
      "encoder.encoder.layer.4.output.dense.weight\n",
      "encoder.encoder.layer.4.output.dense.bias\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.attention.self.query.weight\n",
      "encoder.encoder.layer.5.attention.self.query.bias\n",
      "encoder.encoder.layer.5.attention.self.key.weight\n",
      "encoder.encoder.layer.5.attention.self.key.bias\n",
      "encoder.encoder.layer.5.attention.self.value.weight\n",
      "encoder.encoder.layer.5.attention.self.value.bias\n",
      "encoder.encoder.layer.5.attention.output.dense.weight\n",
      "encoder.encoder.layer.5.attention.output.dense.bias\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.intermediate.dense.weight\n",
      "encoder.encoder.layer.5.intermediate.dense.bias\n",
      "encoder.encoder.layer.5.output.dense.weight\n",
      "encoder.encoder.layer.5.output.dense.bias\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.attention.self.query.weight\n",
      "encoder.encoder.layer.6.attention.self.query.bias\n",
      "encoder.encoder.layer.6.attention.self.key.weight\n",
      "encoder.encoder.layer.6.attention.self.key.bias\n",
      "encoder.encoder.layer.6.attention.self.value.weight\n",
      "encoder.encoder.layer.6.attention.self.value.bias\n",
      "encoder.encoder.layer.6.attention.output.dense.weight\n",
      "encoder.encoder.layer.6.attention.output.dense.bias\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.intermediate.dense.weight\n",
      "encoder.encoder.layer.6.intermediate.dense.bias\n",
      "encoder.encoder.layer.6.output.dense.weight\n",
      "encoder.encoder.layer.6.output.dense.bias\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.attention.self.query.weight\n",
      "encoder.encoder.layer.7.attention.self.query.bias\n",
      "encoder.encoder.layer.7.attention.self.key.weight\n",
      "encoder.encoder.layer.7.attention.self.key.bias\n",
      "encoder.encoder.layer.7.attention.self.value.weight\n",
      "encoder.encoder.layer.7.attention.self.value.bias\n",
      "encoder.encoder.layer.7.attention.output.dense.weight\n",
      "encoder.encoder.layer.7.attention.output.dense.bias\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.intermediate.dense.weight\n",
      "encoder.encoder.layer.7.intermediate.dense.bias\n",
      "encoder.encoder.layer.7.output.dense.weight\n",
      "encoder.encoder.layer.7.output.dense.bias\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.attention.self.query.weight\n",
      "encoder.encoder.layer.8.attention.self.query.bias\n",
      "encoder.encoder.layer.8.attention.self.key.weight\n",
      "encoder.encoder.layer.8.attention.self.key.bias\n",
      "encoder.encoder.layer.8.attention.self.value.weight\n",
      "encoder.encoder.layer.8.attention.self.value.bias\n",
      "encoder.encoder.layer.8.attention.output.dense.weight\n",
      "encoder.encoder.layer.8.attention.output.dense.bias\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.intermediate.dense.weight\n",
      "encoder.encoder.layer.8.intermediate.dense.bias\n",
      "encoder.encoder.layer.8.output.dense.weight\n",
      "encoder.encoder.layer.8.output.dense.bias\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.attention.self.query.weight\n",
      "encoder.encoder.layer.9.attention.self.query.bias\n",
      "encoder.encoder.layer.9.attention.self.key.weight\n",
      "encoder.encoder.layer.9.attention.self.key.bias\n",
      "encoder.encoder.layer.9.attention.self.value.weight\n",
      "encoder.encoder.layer.9.attention.self.value.bias\n",
      "encoder.encoder.layer.9.attention.output.dense.weight\n",
      "encoder.encoder.layer.9.attention.output.dense.bias\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.intermediate.dense.weight\n",
      "encoder.encoder.layer.9.intermediate.dense.bias\n",
      "encoder.encoder.layer.9.output.dense.weight\n",
      "encoder.encoder.layer.9.output.dense.bias\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.attention.self.query.weight\n",
      "encoder.encoder.layer.10.attention.self.query.bias\n",
      "encoder.encoder.layer.10.attention.self.key.weight\n",
      "encoder.encoder.layer.10.attention.self.key.bias\n",
      "encoder.encoder.layer.10.attention.self.value.weight\n",
      "encoder.encoder.layer.10.attention.self.value.bias\n",
      "encoder.encoder.layer.10.attention.output.dense.weight\n",
      "encoder.encoder.layer.10.attention.output.dense.bias\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.intermediate.dense.weight\n",
      "encoder.encoder.layer.10.intermediate.dense.bias\n",
      "encoder.encoder.layer.10.output.dense.weight\n",
      "encoder.encoder.layer.10.output.dense.bias\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.attention.self.query.weight\n",
      "encoder.encoder.layer.11.attention.self.query.bias\n",
      "encoder.encoder.layer.11.attention.self.key.weight\n",
      "encoder.encoder.layer.11.attention.self.key.bias\n",
      "encoder.encoder.layer.11.attention.self.value.weight\n",
      "encoder.encoder.layer.11.attention.self.value.bias\n",
      "encoder.encoder.layer.11.attention.output.dense.weight\n",
      "encoder.encoder.layer.11.attention.output.dense.bias\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.intermediate.dense.weight\n",
      "encoder.encoder.layer.11.intermediate.dense.bias\n",
      "encoder.encoder.layer.11.output.dense.weight\n",
      "encoder.encoder.layer.11.output.dense.bias\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.pooler.dense.weight\n",
      "encoder.pooler.dense.bias\n",
      "decoder.bert.embeddings.word_embeddings.weight\n",
      "decoder.bert.embeddings.position_embeddings.weight\n",
      "decoder.bert.embeddings.token_type_embeddings.weight\n",
      "decoder.bert.embeddings.LayerNorm.weight\n",
      "decoder.bert.embeddings.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.0.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.0.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.0.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.0.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.0.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.0.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.0.output.dense.weight\n",
      "decoder.bert.encoder.layer.0.output.dense.bias\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.1.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.1.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.1.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.1.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.1.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.1.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.1.output.dense.weight\n",
      "decoder.bert.encoder.layer.1.output.dense.bias\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.2.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.2.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.2.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.2.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.2.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.2.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.2.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.2.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.2.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.2.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.2.output.dense.weight\n",
      "decoder.bert.encoder.layer.2.output.dense.bias\n",
      "decoder.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.3.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.3.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.3.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.3.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.3.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.3.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.3.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.3.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.3.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.3.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.3.output.dense.weight\n",
      "decoder.bert.encoder.layer.3.output.dense.bias\n",
      "decoder.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.4.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.4.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.4.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.4.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.4.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.4.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.4.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.4.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.4.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.4.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.4.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.4.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.4.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.4.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.4.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.4.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.4.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.4.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.4.output.dense.weight\n",
      "decoder.bert.encoder.layer.4.output.dense.bias\n",
      "decoder.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.5.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.5.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.5.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.5.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.5.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.5.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.5.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.5.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.5.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.5.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.5.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.5.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.5.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.5.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.5.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.5.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.5.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.5.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.5.output.dense.weight\n",
      "decoder.bert.encoder.layer.5.output.dense.bias\n",
      "decoder.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.6.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.6.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.6.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.6.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.6.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.6.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.6.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.6.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.6.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.6.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.6.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.6.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.6.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.6.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.6.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.6.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.6.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.6.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.6.output.dense.weight\n",
      "decoder.bert.encoder.layer.6.output.dense.bias\n",
      "decoder.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.7.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.7.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.7.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.7.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.7.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.7.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.7.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.7.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.7.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.7.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.7.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.7.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.7.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.7.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.7.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.7.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.7.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.7.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.7.output.dense.weight\n",
      "decoder.bert.encoder.layer.7.output.dense.bias\n",
      "decoder.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.8.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.8.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.8.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.8.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.8.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.8.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.8.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.8.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.8.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.8.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.8.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.8.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.8.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.8.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.8.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.8.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.8.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.8.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.8.output.dense.weight\n",
      "decoder.bert.encoder.layer.8.output.dense.bias\n",
      "decoder.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.9.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.9.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.9.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.9.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.9.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.9.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.9.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.9.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.9.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.9.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.9.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.9.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.9.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.9.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.9.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.9.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.9.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.9.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.9.output.dense.weight\n",
      "decoder.bert.encoder.layer.9.output.dense.bias\n",
      "decoder.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.10.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.10.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.10.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.10.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.10.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.10.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.10.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.10.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.10.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.10.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.10.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.10.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.10.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.10.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.10.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.10.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.10.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.10.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.10.output.dense.weight\n",
      "decoder.bert.encoder.layer.10.output.dense.bias\n",
      "decoder.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.11.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.11.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.11.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.11.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.11.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.11.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.11.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.11.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.11.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.11.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.11.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.11.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.11.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.11.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.11.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.11.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.11.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.11.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.11.output.dense.weight\n",
      "decoder.bert.encoder.layer.11.output.dense.bias\n",
      "decoder.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "decoder.cls.predictions.bias\n",
      "decoder.cls.predictions.transform.dense.weight\n",
      "decoder.cls.predictions.transform.dense.bias\n",
      "decoder.cls.predictions.transform.LayerNorm.weight\n",
      "decoder.cls.predictions.transform.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in bert2bert.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581562d5-86df-412e-9130-fcbe75696f8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these parameters require grad: 114 layers \n",
      "decoder.bert.embeddings.word_embeddings.weight\n",
      "decoder.bert.embeddings.position_embeddings.weight\n",
      "decoder.bert.embeddings.token_type_embeddings.weight\n",
      "decoder.bert.embeddings.LayerNorm.weight\n",
      "decoder.bert.embeddings.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.0.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.0.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.0.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.0.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.0.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.0.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.0.output.dense.weight\n",
      "decoder.bert.encoder.layer.0.output.dense.bias\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.1.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.1.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.1.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.1.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.1.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.1.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.1.output.dense.weight\n",
      "decoder.bert.encoder.layer.1.output.dense.bias\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.2.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.2.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.2.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.2.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.2.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.2.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.2.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.2.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.2.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.2.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.2.output.dense.weight\n",
      "decoder.bert.encoder.layer.2.output.dense.bias\n",
      "decoder.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.3.attention.self.query.weight\n",
      "decoder.bert.encoder.layer.3.attention.self.query.bias\n",
      "decoder.bert.encoder.layer.3.attention.self.key.weight\n",
      "decoder.bert.encoder.layer.3.attention.self.key.bias\n",
      "decoder.bert.encoder.layer.3.attention.self.value.weight\n",
      "decoder.bert.encoder.layer.3.attention.self.value.bias\n",
      "decoder.bert.encoder.layer.3.attention.output.dense.weight\n",
      "decoder.bert.encoder.layer.3.attention.output.dense.bias\n",
      "decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.self.query.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.self.query.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.self.key.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.self.key.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.self.value.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.self.value.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.output.dense.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.output.dense.bias\n",
      "decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias\n",
      "decoder.bert.encoder.layer.3.intermediate.dense.weight\n",
      "decoder.bert.encoder.layer.3.intermediate.dense.bias\n",
      "decoder.bert.encoder.layer.3.output.dense.weight\n",
      "decoder.bert.encoder.layer.3.output.dense.bias\n",
      "decoder.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "decoder.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "decoder.cls.predictions.bias\n",
      "decoder.cls.predictions.transform.dense.weight\n",
      "decoder.cls.predictions.transform.dense.bias\n",
      "decoder.cls.predictions.transform.LayerNorm.weight\n",
      "decoder.cls.predictions.transform.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "# freeze the whole decoder model\n",
    "require_grad_count = 0\n",
    "\n",
    "for name, param in bert2bert.named_parameters():\n",
    "    # if name in bert2bert_new_initialized_weights or name[:11] == 'decoder.cls':\n",
    "    if name[:7] == 'decoder':  \n",
    "        # print(name , ' NOT freeze')\n",
    "        param.requires_grad = True\n",
    "        require_grad_count += 1\n",
    "    else:\n",
    "        # print('freezing top layers ', name)\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# check model architecture\n",
    "print(f'these parameters require grad: {require_grad_count} layers ')\n",
    "for name, param in bert2bert.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeed47e1-6216-4222-baf7-c8a6cf94e0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "clss tensor([[  0,  21,  38,  56,  83,  96, 134, 159, 170, 186, 195, 238, 253, 286,\n",
      "         309, 340, 350, 374, 390, 407, 422,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1],\n",
      "        [  0,  24,  57,  91, 110, 126, 150, 164, 179, 192, 204, 215, 229, 254,\n",
      "         266, 295, 314, 341, 371, 387, 407, 428, 451,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1],\n",
      "        [  0,  19,  39,  60,  88, 109, 126, 155, 174, 186, 201, 224, 257, 266,\n",
      "         290, 332, 352, 366, 388, 397, 420, 441, 456, 492, 524, 543, 564, 580,\n",
      "         590, 609, 627, 644, 664, 685, 701, 728, 750, 772, 781, 795, 816, 841,\n",
      "         855, 864, 880]])\n",
      "token_len_src tensor([435, 469, 512])\n",
      "token_len_tgt tensor([ 40, 100,  50])\n",
      "sent_len tensor([21, 23, 45])\n",
      "padding_token tensor([-1, -1, -1])\n",
      "segs tensor([[ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ...,  1,  1,  1]])\n",
      "src tensor([[  101,  5519,  5208,  ...,    -1,    -1,    -1],\n",
      "        [  101,  6159, 24246,  ...,    -1,    -1,    -1],\n",
      "        [  101,  1006, 13229,  ...,  2062, 10831,  1010]])\n",
      "src_sent_labels tensor([[ 0,  1,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tgt tensor([[    1,  9092, 24032,  1998, 17214, 12186,  4482,  1010,  2119,  2861,\n",
      "          1010,  2020,  2730,  1999,  3050,  3349,     3,  2009,  2003,  3373,\n",
      "          2027,  2020,  2718,  2043,  2037,  2954, 13439,  2041,  2006,  2000,\n",
      "          1996,  2346,     3,  2610,  2024,  2085,  5933,  1996,  4062,     2,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1],\n",
      "        [    1, 24246,  1005,  2015,  2836,  2114,  3304,  2038,  4055,  5448,\n",
      "          2004,  1996, 11854,  2506,  2010,  2088,  2452,  4577, 14734,     3,\n",
      "          1036,  2339,  2052,  1045,  2514,  2026,  2173,  1999,  1996,  2136,\n",
      "          2003, 12361,  1029,  1045,  2147,  2524,  2000,  3046,  1998,  2131,\n",
      "          2046,  2008,  2136,  1012,  1045,  1005,  3726,  2196,  2056,  2026,\n",
      "          2173,  2003, 12361,  1012,  1045,  2079,  1050, 29618,  2102,  5987,\n",
      "          2000,  2377,  1010,  1005,  2002,  2056,     3,  5795,  6060, 26107,\n",
      "          2003,  9657, 23060, 27266,  2063, 29624,  7507, 21784, 15987,  2097,\n",
      "          2022,  4906,  2005,  1996, 24514,  2177,  1040,  8087,     3, 24246,\n",
      "         14456,  1996,  2208,  2003,  2079, 29624,  2953, 29624, 10265,     2],\n",
      "        [    1,  2043,  2016,  2587,  4906,  3842,  1010, 21581, 17741,  8078,\n",
      "          2770,  1010, 28899,  1010,  5742,  2894,     3,  2085,  1010, 17741,\n",
      "          3216,  1998, 18105,  2007,  2014,  3129,  1010,  2515,  3997,  2731,\n",
      "          2007,  2014,  2684,     3,  2016,  2036,  2038,  2195,  7850,  1998,\n",
      "         10779,  2015,  2040,  2031,  3271,  2014,  2247,  1996,  2126,     2,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1]])\n"
     ]
    }
   ],
   "source": [
    "# %reset\n",
    "import data # data.py from local directory\n",
    "# from data import *\n",
    "from data import batch2BertPadId, createAttMask, collate_fn, SummarisationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6215380-c40d-4f9c-8f23-a29817bf2140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length =  287083\n",
      "dict_keys(['clss', 'token_len_src', 'token_len_tgt', 'sent_len', 'padding_token', 'segs', 'src', 'src_sent_labels', 'tgt'])\n",
      "435\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# test dataset class\n",
    "train_file = './preprocessed/train_data.hdf5'\n",
    "test_file = './preprocessed/test_data.hdf5'\n",
    "valid_file ='./preprocessed/valid_data.hdf5'\n",
    "enc_maxlength = 512\n",
    "dec_maxlength = 512\n",
    "\n",
    "train_dataset = SummarisationDataset(path = train_file)\n",
    "print('train_dataset length = ', len(train_dataset))\n",
    "\n",
    "print(train_dataset[0].keys())\n",
    "# print(bert_dataset[0]['src'])\n",
    "print(train_dataset[0]['token_len_src'])\n",
    "\n",
    "batch_list = [train_dataset[idx] for idx in range(3)]\n",
    "print(len(batch_list))\n",
    "# print(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5412d130-25f5-41c2-9908-c9e0c1877b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198933\n",
      "[  101  2410 29627  8889  9765  1010]\n",
      "[  101  1037  2388  5496  1997 15488]\n",
      "[  101  1037  2088  2162  1045 29624]\n",
      "[ 101 2044 2108 7376 2013 2014]\n",
      "[  101  1006 13229  1007  1999  2014]\n",
      "[ 101 2044 2093 2706 1997 8503]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_dataset = SummarisationDataset(test_file)\n",
    "valid_dataset = SummarisationDataset(valid_file)\n",
    "\n",
    "idx = random.randint(0,len(train_dataset)-1 )\n",
    "print(idx)\n",
    "aa_train = train_dataset.__getitem__(idx)\n",
    "print(aa_train['src'][:6])\n",
    "\n",
    "print(test_dataset.__getitem__(random.randint(0,len(test_dataset)-1 )) ['src'][:6])\n",
    "print(valid_dataset.__getitem__(random.randint(0,len(valid_dataset)-1 )) ['src'][:6])\n",
    "\n",
    "\n",
    "bb = train_dataset.__getitem__(100)\n",
    "print(bb['src'][:6])\n",
    "\n",
    "print(test_dataset.__getitem__(10) ['src'][:6])\n",
    "print(valid_dataset.__getitem__(10) ['src'][:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c2cfaa-81d7-4227-84dc-53b665ff0255",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "clss tensor([[   0,   40,   66,  105,  142,  171,  212,  239,  251,  273,  288,  350,\n",
      "          388,  413,  426,  456,  506,  549,  569,  593,  612,  621,  644,  690,\n",
      "          721,  751,  793,  810,  841,  870,  900,  915,  947,  981, 1012, 1068,\n",
      "         1055, 1097, 1116, 1142, 1165, 1211, 1243, 1266, 1297, 1339, 1396, 1412,\n",
      "         1424, 1437, 1475, 1514, 1548, 1592, 1604, 1627, 1516, 1548, 1568, 1582,\n",
      "         1485, 1511, 1534, 1559, 1582, 1619, 1642, 1663, 1685, 1701, 1711, 1722,\n",
      "         1745, 1758, 1792, 1813, 1854, 1893, 1920, 1947, 1976, 2001, 2020, 2049,\n",
      "         2090, 2116, 2126, 2149, 2182, 2217, 2253, 2273, 2284, 2321, 1607, 1620,\n",
      "         1637, 1655, 1665, 1674],\n",
      "        [   0,   48,   86,  146,  177,  189,  218,  247,  291,  356,  377,  405,\n",
      "          443,  466,  496,  518,  554,  576,  608,  653,  690,  703,  716,  755,\n",
      "          778,  858,  904,  933,  949,  941,  991, 1017, 1051, 1069, 1091, 1119,\n",
      "         1138,  744,  754,  763,  780,  801,  811,  822,  840,  849,  865,  883,\n",
      "          894,  904,  918,  935,  945,  961,  972, 1274, 1290, 1309, 1340, 1360,\n",
      "         1380, 1405, 1435, 1455, 1485, 1516, 1537, 1558, 1572, 1585, 1607, 1633,\n",
      "         1643, 1670, 1687, 1713, 1725, 1761, 1776, 1787, 1809, 1820, 2017, 2043,\n",
      "         2069, 2104, 2123, 2136, 2144, 2036, 1486, 1504, 1515, 1528, 1544, 1553,\n",
      "         1566, 1584, 1607, 1627],\n",
      "        [   0,   33,   63,   88,  118,  156,  189,  215,  246,  270,  295,  320,\n",
      "          334,  373,  396,  428,  447,  467,  498,  516,  549,  567,  585,  606,\n",
      "          625,  645,  660,  690,  710,  733,  719,  739,  762,  801,  841,  864,\n",
      "          884,  893,  912,  940,  972,  991,  999, 1028, 1051, 1074, 1102, 1122,\n",
      "         1154, 1187, 1197, 1219, 1231, 1255, 1282, 1300, 1329, 1340, 1366, 1375,\n",
      "         1387, 1432, 1460, 1487, 1512, 1555, 1573, 1590, 1600, 1626, 1649, 1700,\n",
      "         1723, 1742, 1760, 1773, 1804, 1822, 1845, 1860, 1885, 1905, 1938, 1953,\n",
      "         1979, 1998, 2020, 2048, 2084, 2111, 2131, 2168, 2178, 1652, 1663, 1676,\n",
      "         1692, 1726, 1740, 1758]])\n",
      "token_len_src tensor([512, 512, 512])\n",
      "token_len_tgt tensor([216, 206, 317])\n",
      "sent_len tensor([100, 100, 100])\n",
      "padding_token tensor([-1, -1, -1])\n",
      "segs tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "src tensor([[  101,  1006, 13229,  ...,  2041,  2009,  2001],\n",
      "        [  101,  4205, 27327,  ...,  2015, 29625,  3212],\n",
      "        [  101,  1996,  2158,  ...,  2048,  3060, 29624]])\n",
      "src_sent_labels tensor([[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n",
      "tgt tensor([[    1,  2859, 27785,  2331,  9565,  9466,  2000,  6486,  1010,  2110,\n",
      "          2865,  4311,     3,  1996,  2417,  2892,  2554,  1997,  2859,  2003,\n",
      "          6016, 17732,  1998,  6067,     3,  5606,  2988,  5229,  2408,  1037,\n",
      "          2193,  1997,  5721,     3, 10069,  5343,  4073,  2089,  2022, 22532,\n",
      "          2011,  3522,  3082,  4542,  1010,  2062, 19939,     2,  2841,     3,\n",
      "          3309,  1005,  6633, 17180,  2015,  1005,  7388,  7959, 17819,  2021,\n",
      "         11186,  1036, 19258,  1005,  2875,  6368,     3,  2866, 11514,  3003,\n",
      "         12829,  2521,  4270,  2101,  4081, 10756,  2071,  1036,  4133,  1999,\n",
      "          1996,  3420,  1005,  1999,  7884,  2000,  4468,  2125, 18537,  2111,\n",
      "             2,     3,  2045,  2001,  1037,  2645,  2006,  1996,  6510,  1010,\n",
      "          2021, 24459,  2035,  2461,  1999,  1996,  3802, 19190,  4215,  5234,\n",
      "             2,  2048,  1999, 10975, 27131,  4830, 26214,     2,  2085,  2324,\n",
      "          1010,  2001,  2036,  4375,  2019,  2324, 29624,  9629,  2232,  2451,\n",
      "          2344,     2,  1037,  2047,  2110,  1010,  1998,  1037, 12882,  2006,\n",
      "         23733,  6165,  2071,  4604,  3037,  6165, 23990,  1025,     3,  3145,\n",
      "          1055, 16275,  6043,  1010,  2107,  2004,  2019,  3623,  1999,  2489,\n",
      "          2775, 16302,  1998,  1037,  3013,  1999,  3840,  4171,  1010,  2052,\n",
      "          3465,  2012,  2560,  1001,  1015, 29625,  2575, 24457,  3258, 29624,\n",
      "          2050, 29624, 29100,  1012,     2,  6317,  2000,  2128, 29624,  2378,\n",
      "          6961,  3775,  5867,  1996,  2331,     2,  1012,  2115, 12810, 14291,\n",
      "          2015,  2097,  2147,  6669,  1999, 18168, 12260, 14581,  1010, 20717,\n",
      "          2015,  1998, 12901,  2015,  1012,     2,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1],\n",
      "        [    1,  4205, 27327,  2389,  2102,  1010,  3486,  1010,  2001,  2141,\n",
      "          2044,  2010,  2388,  1011,  1037,  2484, 29624, 29100, 29624, 11614,\n",
      "          2317,  2160,  3095,  2121,  1011,  2018,  2019,  6771,  2007,  2496,\n",
      "          5205,  6969,  8514,  8713,  2072,     3,  9393, 27327,  2389,  2102,\n",
      "          3936,  1996,  6771,  1010,  1998,  2014,  2365,  1005,  2015,  6687,\n",
      "          4270,  1010,  1999,  2337,  2286,  1011, 16880,  1996,  2576,  2088,\n",
      "             3,  4205, 27327,  2389,  2102,  1005,  2015,  5615,  2001,  2061,\n",
      "          2485,  2000,  2343, 11531,  2002,  2001,  9919,  1036,  1996,  2034,\n",
      "          2767,  1005,     3, 27327,  2389,  2102,  2003,  2770,  2005,  7756,\n",
      "          4905,  2236,  1998,  2038,  2170,  2006,  2010,  5615,  1005,  2015,\n",
      "          2814,  1998,  6956,  2005,  2490,     2,  1037,  9768,  2446,  2739,\n",
      "          3485,  3555,  2002,  2018,  1036,  7144,  2205,  2172,  2005,  2086,\n",
      "          1005,  1999,  2019,  3720, 25214,  9353, 11039,  5575,  1010,  2632,\n",
      "          3683, 14854, 19648, 13181,  6216,   999,  1006,  3086,  1012,  3052,\n",
      "         20766,  8043,  3231,   999,  1007,  1012,     3,  1037,  2327,  3803,\n",
      "          3761,  2170,  2032,  1037,  1036, 14205,  4392,  2121,  1005,  1010,\n",
      "          6932,  2720, 12022,  9102,  2000,  3277,  2019,  4854, 14920,  1012,\n",
      "             2,  5852,     3,  2047,  1036,  3565, 29624, 20528,  7076,  1005,\n",
      "          1997, 19857,  1011, 29624,  2114,  2029,  1996, 19857, 14855,  2497,\n",
      "          3849,  2000,  3073,  2210,  3860,  1011, 29624,  2024,  3225,  2000,\n",
      "         11740,  2408,  1996,  2406,  1012,     2,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1],\n",
      "        [    1,  2508,  4656,  4097,  6361,  2006,  4946,  2044,  2108,  4469,\n",
      "         23194,  2098,  2000,  5298,  2013,  2414,  1999,  3380,  2005,  5008,\n",
      "          1996,  2942,  2916,  3003,     3,  4097, 12254,  5905,  1998,  2363,\n",
      "          5585,  2086,  1998,  2351,  1999,  3827,  1999,  2687,     2,  3737,\n",
      "          1029,     2,     3,  4465,  2018,  2180,  1996,  2034,  2275,  2021,\n",
      "          2018,  2196,  2077,  7854,  2014,  6538,     2,  2263,     3,  2002,\n",
      "          2097,  2022,  2635,  2112,  2004,  1037,  2522, 29624, 23663,  2099,\n",
      "          1010,  5094,  2007,  9163,  1999,  1996,  2148,  2137,  2679,     2,\n",
      "         29625,  3960,  2273, 28787,  2024,  3173,  3813,  2006,  1037,  2416,\n",
      "         29624,  9629,  2232,  4519,  2558,  2005,  1996, 20996, 27225,  7088,\n",
      "          6939,  1010,  2021,  9424,  2940,  2003, 26670,  2007, 27648,     2,\n",
      "          9459,  2006,  4394,  1998,  7129,  9757,  2308,  1999,  2710,     2,\n",
      "          4404,  1010,  2038,  2062,  2308,  1998,  2111,  2040,  2024,  7177,\n",
      "          3771,     2, 15214, 18417,  2140,  2065,  2151,  2739, 15748,  2055,\n",
      "         13446, 29624, 26760,  9331,  7776,     3,  2008,  7526,  1010,  2002,\n",
      "          2056,  1010, 15123,  9217,  1037,  2375,  2008,  5942,  1996,  2317,\n",
      "          2160,  2000,  2425,  3519,  2382,  2420,  2077,  8287,  3087,  2013,\n",
      "         21025, 21246,  2080,     2,  2206,  2037,  3289,  2114,  3577,  1010,\n",
      "          2119, 26211,  2368,  1998,  3158,  2566, 11741,  2150,  1996,  2034,\n",
      "          7935,  2867,  2000,  3556,  1999,  2093,  2367,  2088,  2452,  8504,\n",
      "          1006,  2294,  1010,  2230,  1010,  2297,  1007,  1012,     3,  5199,\n",
      "          6187,  7100,  2038,  3195,  1999,  2093,  2088, 10268,  2005,  2660,\n",
      "          1025,  2053,  2060,  2827,  2038,  2589,  2061,  1999,  2062,  2084,\n",
      "          2028,  2977,  1012,     3,  6187,  7100,  1006,  2176,  3289,  1010,\n",
      "          2028,  6509,  1007,  2038,  2085,  2042,  2920,  1999,  5179,  2566,\n",
      "          9358,  1997,  2660,  1005,  2015,  3289,  1006,  1023,  1007,  1999,\n",
      "          2088,  2452,  2381,  1012,     3,  2660,  2031,  2180,  2074,  2048,\n",
      "          1997,  2037,  3025,  2340,  2088,  2452,  3503,  1006,  1048,  2575,\n",
      "          1040,  2509,  1007,  1012,     3,  7935,  2024, 20458,  1999,  2037,\n",
      "          2197,  2340,  2088,  2452,  2177,  2754,  3503,  1010,  3045,  2809,\n",
      "          1998,  5059,  2093,  1012,  2037,  2197,  2177,  2754,  4154,  2234,\n",
      "          2067,  1999,  2807,  6431,  5706,  1012,     2]])\n",
      "label tensor([[    1,  2859, 27785,  2331,  9565,  9466,  2000,  6486,  1010,  2110,\n",
      "          2865,  4311,     3,  1996,  2417,  2892,  2554,  1997,  2859,  2003,\n",
      "          6016, 17732,  1998,  6067,     3,  5606,  2988,  5229,  2408,  1037,\n",
      "          2193,  1997,  5721,     3, 10069,  5343,  4073,  2089,  2022, 22532,\n",
      "          2011,  3522,  3082,  4542,  1010,  2062, 19939,     2,  2841,     3,\n",
      "          3309,  1005,  6633, 17180,  2015,  1005,  7388,  7959, 17819,  2021,\n",
      "         11186,  1036, 19258,  1005,  2875,  6368,     3,  2866, 11514,  3003,\n",
      "         12829,  2521,  4270,  2101,  4081, 10756,  2071,  1036,  4133,  1999,\n",
      "          1996,  3420,  1005,  1999,  7884,  2000,  4468,  2125, 18537,  2111,\n",
      "             2,     3,  2045,  2001,  1037,  2645,  2006,  1996,  6510,  1010,\n",
      "          2021, 24459,  2035,  2461,  1999,  1996,  3802, 19190,  4215,  5234,\n",
      "             2,  2048,  1999, 10975, 27131,  4830, 26214,     2,  2085,  2324,\n",
      "          1010,  2001,  2036,  4375,  2019,  2324, 29624,  9629,  2232,  2451,\n",
      "          2344,     2,  1037,  2047,  2110,  1010,  1998,  1037, 12882,  2006,\n",
      "         23733,  6165,  2071,  4604,  3037,  6165, 23990,  1025,     3,  3145,\n",
      "          1055, 16275,  6043,  1010,  2107,  2004,  2019,  3623,  1999,  2489,\n",
      "          2775, 16302,  1998,  1037,  3013,  1999,  3840,  4171,  1010,  2052,\n",
      "          3465,  2012,  2560,  1001,  1015, 29625,  2575, 24457,  3258, 29624,\n",
      "          2050, 29624, 29100,  1012,     2,  6317,  2000,  2128, 29624,  2378,\n",
      "          6961,  3775,  5867,  1996,  2331,     2,  1012,  2115, 12810, 14291,\n",
      "          2015,  2097,  2147,  6669,  1999, 18168, 12260, 14581,  1010, 20717,\n",
      "          2015,  1998, 12901,  2015,  1012,     2,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1],\n",
      "        [    1,  4205, 27327,  2389,  2102,  1010,  3486,  1010,  2001,  2141,\n",
      "          2044,  2010,  2388,  1011,  1037,  2484, 29624, 29100, 29624, 11614,\n",
      "          2317,  2160,  3095,  2121,  1011,  2018,  2019,  6771,  2007,  2496,\n",
      "          5205,  6969,  8514,  8713,  2072,     3,  9393, 27327,  2389,  2102,\n",
      "          3936,  1996,  6771,  1010,  1998,  2014,  2365,  1005,  2015,  6687,\n",
      "          4270,  1010,  1999,  2337,  2286,  1011, 16880,  1996,  2576,  2088,\n",
      "             3,  4205, 27327,  2389,  2102,  1005,  2015,  5615,  2001,  2061,\n",
      "          2485,  2000,  2343, 11531,  2002,  2001,  9919,  1036,  1996,  2034,\n",
      "          2767,  1005,     3, 27327,  2389,  2102,  2003,  2770,  2005,  7756,\n",
      "          4905,  2236,  1998,  2038,  2170,  2006,  2010,  5615,  1005,  2015,\n",
      "          2814,  1998,  6956,  2005,  2490,     2,  1037,  9768,  2446,  2739,\n",
      "          3485,  3555,  2002,  2018,  1036,  7144,  2205,  2172,  2005,  2086,\n",
      "          1005,  1999,  2019,  3720, 25214,  9353, 11039,  5575,  1010,  2632,\n",
      "          3683, 14854, 19648, 13181,  6216,   999,  1006,  3086,  1012,  3052,\n",
      "         20766,  8043,  3231,   999,  1007,  1012,     3,  1037,  2327,  3803,\n",
      "          3761,  2170,  2032,  1037,  1036, 14205,  4392,  2121,  1005,  1010,\n",
      "          6932,  2720, 12022,  9102,  2000,  3277,  2019,  4854, 14920,  1012,\n",
      "             2,  5852,     3,  2047,  1036,  3565, 29624, 20528,  7076,  1005,\n",
      "          1997, 19857,  1011, 29624,  2114,  2029,  1996, 19857, 14855,  2497,\n",
      "          3849,  2000,  3073,  2210,  3860,  1011, 29624,  2024,  3225,  2000,\n",
      "         11740,  2408,  1996,  2406,  1012,     2,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1],\n",
      "        [    1,  2508,  4656,  4097,  6361,  2006,  4946,  2044,  2108,  4469,\n",
      "         23194,  2098,  2000,  5298,  2013,  2414,  1999,  3380,  2005,  5008,\n",
      "          1996,  2942,  2916,  3003,     3,  4097, 12254,  5905,  1998,  2363,\n",
      "          5585,  2086,  1998,  2351,  1999,  3827,  1999,  2687,     2,  3737,\n",
      "          1029,     2,     3,  4465,  2018,  2180,  1996,  2034,  2275,  2021,\n",
      "          2018,  2196,  2077,  7854,  2014,  6538,     2,  2263,     3,  2002,\n",
      "          2097,  2022,  2635,  2112,  2004,  1037,  2522, 29624, 23663,  2099,\n",
      "          1010,  5094,  2007,  9163,  1999,  1996,  2148,  2137,  2679,     2,\n",
      "         29625,  3960,  2273, 28787,  2024,  3173,  3813,  2006,  1037,  2416,\n",
      "         29624,  9629,  2232,  4519,  2558,  2005,  1996, 20996, 27225,  7088,\n",
      "          6939,  1010,  2021,  9424,  2940,  2003, 26670,  2007, 27648,     2,\n",
      "          9459,  2006,  4394,  1998,  7129,  9757,  2308,  1999,  2710,     2,\n",
      "          4404,  1010,  2038,  2062,  2308,  1998,  2111,  2040,  2024,  7177,\n",
      "          3771,     2, 15214, 18417,  2140,  2065,  2151,  2739, 15748,  2055,\n",
      "         13446, 29624, 26760,  9331,  7776,     3,  2008,  7526,  1010,  2002,\n",
      "          2056,  1010, 15123,  9217,  1037,  2375,  2008,  5942,  1996,  2317,\n",
      "          2160,  2000,  2425,  3519,  2382,  2420,  2077,  8287,  3087,  2013,\n",
      "         21025, 21246,  2080,     2,  2206,  2037,  3289,  2114,  3577,  1010,\n",
      "          2119, 26211,  2368,  1998,  3158,  2566, 11741,  2150,  1996,  2034,\n",
      "          7935,  2867,  2000,  3556,  1999,  2093,  2367,  2088,  2452,  8504,\n",
      "          1006,  2294,  1010,  2230,  1010,  2297,  1007,  1012,     3,  5199,\n",
      "          6187,  7100,  2038,  3195,  1999,  2093,  2088, 10268,  2005,  2660,\n",
      "          1025,  2053,  2060,  2827,  2038,  2589,  2061,  1999,  2062,  2084,\n",
      "          2028,  2977,  1012,     3,  6187,  7100,  1006,  2176,  3289,  1010,\n",
      "          2028,  6509,  1007,  2038,  2085,  2042,  2920,  1999,  5179,  2566,\n",
      "          9358,  1997,  2660,  1005,  2015,  3289,  1006,  1023,  1007,  1999,\n",
      "          2088,  2452,  2381,  1012,     3,  2660,  2031,  2180,  2074,  2048,\n",
      "          1997,  2037,  3025,  2340,  2088,  2452,  3503,  1006,  1048,  2575,\n",
      "          1040,  2509,  1007,  1012,     3,  7935,  2024, 20458,  1999,  2037,\n",
      "          2197,  2340,  2088,  2452,  2177,  2754,  3503,  1010,  3045,  2809,\n",
      "          1998,  5059,  2093,  1012,  2037,  2197,  2177,  2754,  4154,  2234,\n",
      "          2067,  1999,  2807,  6431,  5706,  1012,     2]])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SummarisationDataset(path = train_file)\n",
    "batch_list = [train_dataset[idx] for idx in range(3)]\n",
    "batch = collate_fn(batch_list)\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2911dbf5-ff89-43f5-828c-c2b090b523d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "dict_keys(['clss', 'token_len_src', 'token_len_tgt', 'sent_len', 'padding_token', 'segs', 'src', 'src_sent_labels', 'tgt'])\n",
      "torch.Size([8, 512])\n",
      "tensor([[  101,  1037,  2158,  ...,  2056,  1012,   102],\n",
      "        [  101,  2750,  1037,  ...,  2030,  2178,  2364],\n",
      "        [  101,  1037,  2136,  ...,  1010,  1996,  2317],\n",
      "        ...,\n",
      "        [  101,  1037,  3232,  ..., 10710,  7279, 29431],\n",
      "        [  101,  1037,  5776,  ..., 29623, 19841,  2620],\n",
      "        [  101,  2047,  2259,  ...,  2196,  2042,  2062]])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_dataset = SummarisationDataset(train_file)\n",
    "test_dataset = SummarisationDataset(test_file)\n",
    "valid_dataset = SummarisationDataset(valid_file)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          collate_fn = collate_fn,\n",
    "                          num_workers = 8, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          collate_fn = collate_fn,\n",
    "                          num_workers = 8, shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          collate_fn = collate_fn,\n",
    "                          num_workers = 8, shuffle=True)\n",
    "\n",
    "valid_batch1 = next(iter(valid_dataloader))\n",
    "print(type(valid_batch1['tgt']))\n",
    "print(valid_batch1.keys())\n",
    "print(valid_batch1['src'].shape)\n",
    "print(valid_batch1['src'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d82717d7-d34e-40d6-aa33-070d64cabce4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 2.8590e+03, 2.7785e+04, 2.3310e+03, 9.5650e+03, 9.4660e+03,\n",
      "        2.0000e+03, 6.4860e+03, 1.0100e+03, 2.1100e+03, 2.8650e+03, 4.3110e+03,\n",
      "        3.0000e+00, 1.9960e+03, 2.4170e+03, 2.8920e+03, 2.5540e+03, 1.9970e+03,\n",
      "        2.8590e+03, 2.0030e+03, 6.0160e+03, 1.7732e+04, 1.9980e+03, 6.0670e+03,\n",
      "        3.0000e+00, 5.6060e+03, 2.9880e+03, 5.2290e+03, 2.4080e+03, 1.0370e+03,\n",
      "        2.1930e+03, 1.9970e+03, 5.7210e+03, 3.0000e+00, 1.0069e+04, 5.3430e+03,\n",
      "        4.0730e+03, 2.0890e+03, 2.0220e+03, 2.2532e+04, 2.0110e+03, 3.5220e+03,\n",
      "        3.0820e+03, 4.5420e+03, 1.0100e+03, 2.0620e+03, 1.9939e+04, 2.0000e+00,\n",
      "        2.8410e+03, 3.0000e+00, 3.3090e+03, 1.0050e+03, 6.6330e+03, 1.7180e+04,\n",
      "        2.0150e+03, 1.0050e+03, 7.3880e+03, 7.9590e+03, 1.7819e+04, 2.0210e+03,\n",
      "        1.1186e+04, 1.0360e+03, 1.9258e+04, 1.0050e+03, 2.8750e+03, 6.3680e+03,\n",
      "        3.0000e+00, 2.8660e+03, 1.1514e+04, 3.0030e+03, 1.2829e+04, 2.5210e+03,\n",
      "        4.2700e+03, 2.1010e+03, 4.0810e+03, 1.0756e+04, 2.0710e+03, 1.0360e+03,\n",
      "        4.1330e+03, 1.9990e+03, 1.9960e+03, 3.4200e+03, 1.0050e+03, 1.9990e+03,\n",
      "        7.8840e+03, 2.0000e+03, 4.4680e+03, 2.1250e+03, 1.8537e+04, 2.1110e+03,\n",
      "        2.0000e+00, 3.0000e+00, 2.0450e+03, 2.0010e+03, 1.0370e+03, 2.6450e+03,\n",
      "        2.0060e+03, 1.9960e+03, 6.5100e+03, 1.0100e+03, 2.0210e+03, 2.4459e+04,\n",
      "        2.0350e+03, 2.4610e+03, 1.9990e+03, 1.9960e+03, 3.8020e+03, 1.9190e+04,\n",
      "        4.2150e+03, 5.2340e+03, 2.0000e+00, 2.0480e+03, 1.9990e+03, 1.0975e+04,\n",
      "        2.7131e+04, 4.8300e+03, 2.6214e+04, 2.0000e+00, 2.0850e+03, 2.3240e+03,\n",
      "        1.0100e+03, 2.0010e+03, 2.0360e+03, 4.3750e+03, 2.0190e+03, 2.3240e+03,\n",
      "        2.9624e+04, 9.6290e+03, 2.2320e+03, 2.4510e+03, 2.3440e+03, 2.0000e+00,\n",
      "        1.0370e+03, 2.0470e+03, 2.1100e+03, 1.0100e+03, 1.9980e+03, 1.0370e+03,\n",
      "        1.2882e+04, 2.0060e+03, 2.3733e+04, 6.1650e+03, 2.0710e+03, 4.6040e+03,\n",
      "        3.0370e+03, 6.1650e+03, 2.3990e+04, 1.0250e+03, 3.0000e+00, 3.1450e+03,\n",
      "        1.0550e+03, 1.6275e+04, 6.0430e+03, 1.0100e+03, 2.1070e+03, 2.0040e+03,\n",
      "        2.0190e+03, 3.6230e+03, 1.9990e+03, 2.4890e+03, 2.7750e+03, 1.6302e+04,\n",
      "        1.9980e+03, 1.0370e+03, 3.0130e+03, 1.9990e+03, 3.8400e+03, 4.1710e+03,\n",
      "        1.0100e+03, 2.0520e+03, 3.4650e+03, 2.0120e+03, 2.5600e+03, 1.0010e+03,\n",
      "        1.0150e+03, 2.9625e+04, 2.5750e+03, 2.4457e+04, 3.2580e+03, 2.9624e+04,\n",
      "        2.0500e+03, 2.9624e+04, 2.9100e+04, 1.0120e+03, 2.0000e+00, 6.3170e+03,\n",
      "        2.0000e+03, 2.1280e+03, 2.9624e+04, 2.3780e+03, 6.9610e+03, 3.7750e+03,\n",
      "        5.8670e+03, 1.9960e+03, 2.3310e+03, 2.0000e+00, 1.0120e+03, 2.1150e+03,\n",
      "        1.2810e+04, 1.4291e+04, 2.0150e+03, 2.0970e+03, 2.1470e+03, 6.6690e+03,\n",
      "        1.9990e+03, 1.8168e+04, 1.2260e+04, 1.4581e+04, 1.0100e+03, 2.0717e+04,\n",
      "        2.0150e+03, 1.9980e+03, 1.2901e+04, 2.0150e+03, 1.0120e+03, 2.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "tensor(284) tensor(284)\n"
     ]
    }
   ],
   "source": [
    "# convert -1 to pad_token_id of bert tokenizer\n",
    "\n",
    "tgt1 = batch2BertPadId(train_batch1['tgt'])\n",
    "mask_tgt1 = createAttMask(train_batch1['tgt'])\n",
    "\n",
    "print(tgt1[0,:])\n",
    "print(mask_tgt1[0,:])\n",
    "\n",
    "print(sum(tgt1[0,:] == 0), sum(mask_tgt1[0,:] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "109e3fe8-ed95-4c83-a4d1-6788a6d03baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c879f9-d7a0-4f77-b787-87da3d054ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5679ac-6e33-4989-9ba4-4224998888e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tgt1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5631/1119102009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tgt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tgt1' is not defined"
     ]
    }
   ],
   "source": [
    "print(tz.decode(tgt1[0,:256]))\n",
    "print(tz.decode(tgt1[1,:256]))\n",
    "\n",
    "print(tz.decode(train_batch1['tgt'][0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547d9bb7-0b62-462d-be08-9431eecacf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "# pad_id = 0\n",
    "optimizer = torch.optim.Adam(bert2bert.parameters(), lr=0.0001)\n",
    "loss_fct = nn.CrossEntropyLoss(ignore_index = -100)\n",
    "criterion = nn.NLLLoss(ignore_index = -100)\n",
    "bert2bert = bert2bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f788adbc-8a36-453f-aebb-ab4c495be72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert2bert.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27f9c782-fa4e-4a67-8d50-4e47c2a5cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_loss(prediction_scores, labels):\n",
    "    \"\"\"Compute our custom loss\"\"\"\n",
    "    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() # prediction_scores is logits\n",
    "    labels = labels[:, 1:].contiguous()\n",
    "    loss_fct = nn.CrossEntropyLoss(ignore_index = -100)\n",
    "    lm_loss = loss_fct(shifted_prediction_scores.view(-1, bert2bert.config.decoder.vocab_size), \n",
    "                       labels.view(-1))\n",
    "\n",
    "    return lm_loss\n",
    "\n",
    "def compute_loss2(prediction_scores, labels):\n",
    "    \"\"\"Compute our custom loss\"\"\"\n",
    "    predictions = F.softmax(prediction_scores)\n",
    "    predictions = predictions[:, :-1, :].contiguous()\n",
    "    labels = labels[:, 1:]\n",
    "\n",
    "    rearranged_output = predictions.view(predictions.shape[0]*predictions.shape[1], -1)\n",
    "    rearranged_target = labels.contiguous().view(-1)\n",
    "\n",
    "    lm_loss = criterion(rearranged_output, rearranged_target)\n",
    "\n",
    "    return lm_loss\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bf37d0-3e80-4076-8dbe-805addbc6c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one train loop\n",
    "\n",
    "def train_model():\n",
    "    bert2bert.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        src, tgt = batch['src'], batch['tgt'] # \n",
    "\n",
    "        # convert -1 to 0\n",
    "        src, tgt = batch2BertPadId(src), batch2BertPadId(tgt) # .type(LongTensor)\n",
    "\n",
    "        # create label\n",
    "        labels = tgt.clone()\n",
    "        labels[labels == 0] = -100 # by default loss function ignore id = -100\n",
    "\n",
    "        # create mask\n",
    "        src_att_mask = createAttMask(src)\n",
    "        tgt_att_mask = createAttMask(tgt)\n",
    "\n",
    "        src, tgt, labels = src.to(device), tgt.to(device), labels.to(device)\n",
    "        src_att_mask, tgt_att_mask = src_att_mask.to(device), tgt_att_mask.to(device)\n",
    "\n",
    "        out = bert2bert(input_ids=src, attention_mask=src_att_mask,\n",
    "                        decoder_input_ids=tgt, decoder_attention_mask=tgt_att_mask,\n",
    "                        labels=labels)\n",
    "        loss, prediction_scores = out.loss, out.logits\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert2bert.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"Mean training epoch loss:\", (epoch_loss / len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16da517d-9875-4d38-995c-e8bfee7fcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    bert2bert.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(valid_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src, tgt = batch['src'], batch['tgt'] # \n",
    "        # convert -1 to 0\n",
    "        src, tgt = batch2BertPadId(src), batch2BertPadId(tgt) # .type(LongTensor)\n",
    "        # create label\n",
    "        labels = tgt.clone()\n",
    "        labels[labels == 0] = -100 # by default loss function ignore id = -100\n",
    "        # create mask\n",
    "        src_att_mask = createAttMask(src)\n",
    "        tgt_att_mask = createAttMask(tgt)\n",
    "\n",
    "        src, tgt, labels = src.to(device), tgt.to(device), labels.to(device)\n",
    "        src_att_mask, tgt_att_mask = src_att_mask.to(device), tgt_att_mask.to(device)\n",
    "\n",
    "        out = bert2bert(input_ids=src, attention_mask=src_att_mask,\n",
    "                        decoder_input_ids=tgt, decoder_attention_mask=tgt_att_mask,\n",
    "                        labels=labels)\n",
    "        loss, prediction_scores = out.loss, out.logits\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Mean validation epoch loss:\", (epoch_loss / len(valid_dataloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a373765-b1c8-4af9-ac27-69a30dc08896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 35886/35886 [58:12<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training epoch loss: 2.7072609502728526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 1671/1671 [01:48<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation epoch loss: 2.561582028402104\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████| 35886/35886 [58:09<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training epoch loss: 2.6317593583041248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 1671/1671 [01:48<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation epoch loss: 2.5293476005858966\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████| 35886/35886 [58:14<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training epoch loss: 2.5729181375224766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 1671/1671 [01:48<00:00, 15.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation epoch loss: 2.5062586908779907\n",
      "Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59207/1280998058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving model ..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msave_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "# MAIN TRAINING LOOP\n",
    "NUM_EPOCHS = 6\n",
    "save_location = './models/'\n",
    "model_name = 'bert2bert.pt'\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Starting epoch\", epoch+1)\n",
    "    train_model()\n",
    "    eval_model()\n",
    "\n",
    "print(\"Saving model ..\")\n",
    "if not os.path.exists(save_location):\n",
    "    os.makedirs(save_location)\n",
    "save_location = os.path.join(save_location, model_name)\n",
    "torch.save(bert2bert, save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2344f3e5-c9c7-4e2f-b0d5-3fa2397aaa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model ..\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model ..\")\n",
    "if not os.path.exists(save_location):\n",
    "    os.makedirs(save_location)\n",
    "save_location = os.path.join(save_location, model_name)\n",
    "torch.save(bert2bert, save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc7393f-dfd3-4b36-a0ce-bc4c6f3178e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert_checkpoint = { \n",
    "    'epoch': epoch,\n",
    "    'model': bert2bert.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(bert2bert_checkpoint, './models/bert2bert_checkpoint.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660b2486-ab33-4394-86df-3e5fe89b9670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1671/1671 [01:46<00:00, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss: 3.9672808475939547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9c749-ccd9-46d0-b2b4-7a5ca62774bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f13e5e7c-3e37-43e9-9eae-27f13c13b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "690d8476-1939-4af2-af5c-0940a377df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['clss', 'token_len_src', 'token_len_tgt', 'sent_len', 'padding_token', 'segs', 'src', 'src_sent_labels', 'tgt'])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11838df0-fcdc-4d82-b6f7-f6d2c5838832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.511220932006836\n"
     ]
    }
   ],
   "source": [
    "bert2bert.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "epoch_loss = 0\n",
    "\n",
    "src, tgt = batch['src'], batch['tgt'] # \n",
    "\n",
    "# convert -1 to 0\n",
    "src, tgt = batch2BertPadId(src), batch2BertPadId(tgt) # .type(LongTensor)\n",
    "# create label\n",
    "labels = tgt.clone()\n",
    "labels[labels == 0] = -100\n",
    "\n",
    "# print('tgt = ', tgt)\n",
    "# print('label = ', labels)\n",
    "\n",
    "# create mask\n",
    "src_att_mask = createAttMask(src)\n",
    "tgt_att_mask = createAttMask(tgt)\n",
    "\n",
    "src, tgt, labels = src.to(device), tgt.to(device), labels.to(device)\n",
    "src_att_mask, tgt_att_mask = src_att_mask.to(device), tgt_att_mask.to(device)\n",
    "\n",
    "out = bert2bert(input_ids=src, attention_mask=src_att_mask,\n",
    "                decoder_input_ids=tgt, decoder_attention_mask=tgt_att_mask,\n",
    "                labels=labels)\n",
    "\n",
    "loss = out[0]\n",
    "prediction_scores = out[1]\n",
    "\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(bert2bert.parameters(), 1.0)\n",
    "optimizer.step()\n",
    "\n",
    "epoch_loss += loss.item()\n",
    "\n",
    "print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c361580e-8adb-4c0b-bac5-8c2091bc1ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6513, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09d54a3c-65e3-4f98-8761-77509148243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 75, 30522])\n",
      "tensor(9.9302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "-0.10400070995092392\n",
      "tensor(-0.1040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([8, 75])\n",
      "torch.Size([8, 75, 30522])\n"
     ]
    }
   ],
   "source": [
    "loss, logits = out.loss, out.logits\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(epoch_loss)\n",
    "print(loss2)\n",
    "\n",
    "print(tgt.shape)\n",
    "print(prediction_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c1a6165-f54f-4ff7-b454-89e8d7fe36c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 75, 30522])\n",
      "tensor(9.9302, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30327/1015387509.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert2bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "\n",
    "loss, logits = out.loss, out.logits\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(bert2bert.parameters(), 1.0)\n",
    "optimizer.step()\n",
    "\n",
    "epoch_loss += loss.item()\n",
    "\n",
    "print(epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
